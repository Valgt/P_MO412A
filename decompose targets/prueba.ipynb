{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (0.13.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: jinja2 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: wrapt in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 43.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: scipy in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: Pillow in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: click in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\mestrado\\2024-2\\algoritmos en grafos\\p_mo412a\\p_mo412a\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sgsr_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sgsr_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instalar spaCy y descargar el modelo de inglés\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Instalar SentenceTransformers para embeddings semánticos\n",
    "!pip install sentence-transformers\n",
    "\n",
    "# Instalar NLTK para manejo de sinónimos\n",
    "!pip install nltk\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV cargado exitosamente.\n",
      "\n",
      "Se han extraído 19 textos para el goal_id 17.\n",
      "\n",
      "Lista 'targets':\n",
      "1. Strengthen domestic resource mobilization, including through international support to developing countries, to improve domestic capacity for tax and other revenue collection | Total government revenue as a proportion of GDP, by source | Proportion of domestic budget funded by domestic taxes\n",
      "\n",
      "2. Developed countries to implement fully their official development assistance commitments, including the commitment by many developed countries to achieve the target of 0.7 per cent of ODA/GNI to developing countries and 0.15 to 0.20 per cent of ODA/GNI to least developed countries; ODA providers are encouraged to consider setting a target to provide at least 0.20 per cent of ODA/GNI to least developed countries | Net official development assistance, total and to least developed countries, as a proportion of the Organization for Economic Cooperation and Development (OECD) Development Assistance Committee donors' gross national income (GNI)\n",
      "\n",
      "3. Mobilize additional financial resources for developing countries from multiple sources | Foreign direct investments (FDI), official development assistance and South- South Cooperation as a proportion of total domestic budget | Volume of remittances (in United States dollars) as a proportion of total GDP\n",
      "\n",
      "4. Assist developing countries in attaining long-term debt sustainability through coordinated policies aimed at fostering debt financing, debt relief and debt restructuring, as appropriate, and address the external debt of highly indebted poor countries to reduce debt distress | Debt service as a proportion of exports of goods and services\n",
      "\n",
      "5. Adopt and implement investment promotion regimes for least developed countries  | Number of countries that adopt and implement investment promotion regimes for least developed countries\n",
      "\n",
      "6. Enhance North-South, South-South and triangular regional and international cooperation on and access to science, technology and innovation and enhance knowledge sharing on mutually agreed terms, including through improved coordination among existing mechanisms, in particular at the United Nations level, and through a global technology facilitation mechanism | Number of science and/or technology cooperation agreements and programmes between countries, by type of cooperation | Fixed Internet broadband subscriptions per 100 inhabitants, by speed\n",
      "\n",
      "7. Promote the development, transfer, dissemination and diffusion of environmentally sound technologies to developing countries on favourable terms, including on concessional and preferential terms, as mutually agreed | Total amount of approved funding for developing countries to promote the development, transfer, dissemination and diffusion of environmentally sound technologies\n",
      "\n",
      "8. Fully operationalize the technology bank and science, technology and innovation capacity-building mechanism for least developed countries by 2017 and enhance the use of enabling technology, in particular information and communications technology  | Proportion of individuals using the Internet\n",
      "\n",
      "9. Enhance international support for implementing effective and targeted capacity-building in developing countries to support national plans to implement all the sustainable development goals, including through North-South, South-South and triangular cooperation  | Dollar value of financial and technical assistance (including through North-South, South-South and triangular cooperation) committed to developing countries\n",
      "\n",
      "10. Promote a universal, rules-based, open, non-discriminatory and equitable multilateral trading system under the World Trade Organization, including through the conclusion of negotiations under its Doha Development Agenda | Worldwide weighted tariff- average\n",
      "\n",
      "11. Significantly increase the exports of developing countries, in particular with a view to doubling the least developed countries’ share of global exports by 2020 | Developing countries' and least developed countries' share of global exports\n",
      "\n",
      "12. Realize timely implementation of duty-free and quota-free market access on a lasting basis for all least developed countries, consistent with World Trade Organization decisions, including by ensuring that preferential rules of origin applicable to imports from least developed countries are transparent and simple, and contribute to facilitating market access  | Average tariffs faced by developing countries, least developed countries and small island developing States\n",
      "\n",
      "13. Enhance global macroeconomic stability, including through policy coordination and policy coherence | Macroeconomic Dashboard\n",
      "\n",
      "14. Enhance policy coherence for sustainable development | Number of countries with mechanisms in place to enhance policy coherence of sustainable development\n",
      "\n",
      "15. Respect each country’s policy space and leadership to establish and implement policies for poverty eradication and sustainable development | Extent of use of country-owned results frameworks and planning tools by providers of development cooperation\n",
      "\n",
      "16. Enhance the global partnership for sustainable development, complemented by multi-stakeholder partnerships that mobilize and share knowledge, expertise, technology and financial resources, to support the achievement of the sustainable development goals in all countries, in particular developing countries | Number of countries reporting progress in multi-stakeholder development effectiveness monitoring frameworks that support the achievement of the sustainable development goals\n",
      "\n",
      "17. Encourage and promote effective public, public-private and civil society partnerships, building on the experience and resourcing strategies of partnerships | Amount of United States dollars committed to public-private and civil society partnerships\n",
      "\n",
      "18. By 2020, enhance capacity-building support to developing countries, including for least developed countries and small island developing States, to increase significantly the availability of high-quality, timely and reliable data disaggregated by income, gender, age, race, ethnicity, migratory status, disability, geographic location and other characteristics relevant in national contexts | Proportion of sustainable development indicators produced at the national level with full disaggregation when relevant to the target, in accordance with the Fundamental Principles of Official Statistics | Number of countries that have national statistical legislation that complies with the Fundamental Principles of Official Statistics | Number of countries with a national statistical plan that is fully funded and under implementation, by source of funding\n",
      "\n",
      "19. By 2030, build on existing initiatives to develop measurements of progress on sustainable development that complement gross domestic product, and support statistical capacity-building in developing countries  | Dollar value of all resources made available to strengthen statistical capacity in developing countries | Proportion of countries that (a) have conducted at least one population and housing census in the last 10 years; and (b) have achieved 100 per cent birth registration and 80 per cent death registration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nombre del archivo CSV generado previamente\n",
    "archivo_csv = 'SDG_final.csv'\n",
    "\n",
    "# Definir el goal_id fijo\n",
    "goal_id_fijo = '17'  # Cambia este valor según el GOAL que desees (por ejemplo, '2', '3', etc.)\n",
    "\n",
    "try:\n",
    "    # Cargar el archivo CSV\n",
    "    df = pd.read_csv(archivo_csv, delimiter=',', encoding='utf-8')\n",
    "    print(\"Archivo CSV cargado exitosamente.\\n\")\n",
    "    \n",
    "    # Verificar que las columnas necesarias existen en el DataFrame\n",
    "    columnas_necesarias = {'goal_id', 'target_id', 'texto'}\n",
    "    if not columnas_necesarias.issubset(df.columns):\n",
    "        missing = columnas_necesarias - set(df.columns)\n",
    "        raise KeyError(f\"Faltan las siguientes columnas en el archivo CSV: {missing}\")\n",
    "    \n",
    "    # Filtrar las filas que corresponden al goal_id especificado\n",
    "    df_goal = df[df['goal_id'].astype(str) == str(goal_id_fijo)]\n",
    "    \n",
    "    # Verificar si se encontraron filas para el goal_id especificado\n",
    "    if df_goal.empty:\n",
    "        print(f\"No se encontraron registros para el goal_id '{goal_id_fijo}'.\")\n",
    "    else:\n",
    "        # Extraer la columna 'texto' y almacenarla en una lista llamada 'targets'\n",
    "        targets = df_goal['texto'].tolist()\n",
    "        goal_ids = df_goal['goal_id'].tolist()\n",
    "        target_ids = df_goal['target_id'].tolist()\n",
    "        \n",
    "        # Mostrar la cantidad de textos extraídos\n",
    "        print(f\"Se han extraído {len(targets)} textos para el goal_id {goal_id_fijo}.\\n\")\n",
    "        \n",
    "        # Mostrar los textos extraídos\n",
    "        print(\"Lista 'targets':\")\n",
    "        for idx, texto in enumerate(targets, start=1):\n",
    "            print(f\"{idx}. {texto}\\n\")\n",
    "        \n",
    "        # (Opcional) Guardar la lista 'targets' en un archivo de texto\n",
    "        # Descomenta las siguientes líneas si deseas guardar los textos en un archivo\n",
    "        \"\"\"\n",
    "        nombre_archivo_salida = f'targets_goal_{goal_id_fijo}.txt'\n",
    "        with open(nombre_archivo_salida, 'w', encoding='utf-8') as f:\n",
    "            for texto in targets:\n",
    "                f.write(texto + '\\n')\n",
    "        print(f\"Lista 'targets' guardada en el archivo '{nombre_archivo_salida}'.\")\n",
    "        \"\"\"\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{archivo_csv}' no se encontró en el directorio actual.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Error: El archivo '{archivo_csv}' está vacío.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(f\"Error: El archivo '{archivo_csv}' no pudo ser parseado. Verifica el delimitador y el formato del archivo.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error de clave: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'Compiled-Keywords-for-SDG.csv' cargado exitosamente.\n",
      "\n",
      "Lista 'palabras_clave' para la columna 'SDG 17' y 'Misc':\n",
      "\n",
      "1. Capacity building\n",
      "2. Civil society partnerships\n",
      "3. Communication technologies\n",
      "4. Debt sustainability\n",
      "5. Development assistance\n",
      "6. Disaggregated data\n",
      "7. Doha Development Agenda\n",
      "8. Entrepreneurship\n",
      "9. Environmentally sound technologies\n",
      "10. Foreign direct investments\n",
      "11. Fostering innovation\n",
      "12. Free trade\n",
      "13. Fundamental principles of official statistics\n",
      "14. Global partnership\n",
      "15. Global partnership for sustainable development\n",
      "16. Global stability\n",
      "17. International aid\n",
      "18. International cooperation\n",
      "19. International population and housing census\n",
      "20. International support\n",
      "21. International support for developing countries\n",
      "22. Knowledge sharing\n",
      "23. Multi-stakeholder partnerships\n",
      "24. Poverty eradication\n",
      "25. Public-private partnerships\n",
      "26. Science cooperation agreements\n",
      "27. Technology cooperation agreements\n",
      "28. Technology transfer\n",
      "29. Weighted tariff average\n",
      "30. Women entrepreneurs\n",
      "31. World Trade Organization\n",
      "32. Accountability\n",
      "33. Alternative energy\n",
      "34. Biodiversity\n",
      "35. Caring for country\n",
      "36. CO2 emissions\n",
      "37. Developing countries\n",
      "38. Disability\n",
      "39. Eco tourism\n",
      "40. Ecology\n",
      "41. Energy efficiency\n",
      "42. Environment\n",
      "43. Environmental\n",
      "44. Environmental degradation\n",
      "45. Environmental policy\n",
      "46. Environmental sustainability\n",
      "47. Equal rights to economic resources\n",
      "48. Ethical\n",
      "49. Food-energy-water nexus / Water-energy-food nexus\n",
      "50. Forced displacement\n",
      "51. Good governance\n",
      "52. Governance\n",
      "53. Governance and policy\n",
      "54. Governance and risk\n",
      "55. Human rights\n",
      "56. Human well-being\n",
      "57. Inclusive\n",
      "58. Indigenous knowledge\n",
      "59. Inter-generational\n",
      "60. Irregular migration\n",
      "61. Kaitiakitanga\n",
      "62. Land locked developing countries\n",
      "63. Least developed countries\n",
      "64. Leave no one behind\n",
      "65. Low impact agriculture\n",
      "66. Low impact farming\n",
      "67. Low impact horticulture\n",
      "68. Migrant rights\n",
      "69. Migration and policy\n",
      "70. Policy coherence\n",
      "71. Pollution - Air/Soil/Water\n",
      "72. Promotion of shared responsibilities\n",
      "73. Recycling\n",
      "74. Refugee crisis\n",
      "75. Refugee rights\n",
      "76. Renewable\n",
      "77. Resilient\n",
      "78. Reuse technologies\n",
      "79. Small island developing states\n",
      "80. Smart cities\n",
      "81. Smart grid\n",
      "82. Smart houses\n",
      "83. Social protection policies\n",
      "84. Social responsibility\n",
      "85. Sustainability\n",
      "86. Sustainable\n",
      "87. Sustainable development\n",
      "88. Sustainable Development Goals\n",
      "89. Sustainable development indicators\n",
      "90. Sustainable management\n",
      "91. Sustainable public transport\n",
      "92. Sustainable societies\n",
      "93. Sustainable transport\n",
      "94. Technology for sustainable development\n",
      "95. Tele-working\n",
      "96. Transboundary cooperation\n",
      "97. Water sensitive revitalisation\n",
      "98. Well-being\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nombre del archivo CSV\n",
    "archivo_csv = 'Compiled-Keywords-for-SDG.csv'\n",
    "\n",
    "# Definir el parámetro fijo X (por ejemplo, 'SDG 1')\n",
    "X = f'SDG {goal_id_fijo}'  # Cambia este valor según el GOAL que desees (e.g., 'SDG 2', 'SDG3', etc.)\n",
    "\n",
    "try:\n",
    "    # Cargar el archivo CSV\n",
    "    df = pd.read_csv(archivo_csv, delimiter=',', encoding='utf-8')\n",
    "    print(f\"Archivo '{archivo_csv}' cargado exitosamente.\\n\")\n",
    "    \n",
    "    # Verificar que las columnas X y 'Misc' existen en el DataFrame\n",
    "    columnas_necesarias = [X, 'Misc']\n",
    "    columnas_presentes = df.columns.tolist()\n",
    "    \n",
    "    for columna in columnas_necesarias:\n",
    "        if columna not in columnas_presentes:\n",
    "            raise KeyError(f\"La columna '{columna}' no existe en el archivo CSV. Columnas disponibles: {columnas_presentes}\")\n",
    "    \n",
    "    # Extraer las columnas X y 'Misc', eliminando valores nulos y espacios en blanco\n",
    "    elementos_X = df[X].dropna().astype(str).str.strip()\n",
    "    elementos_Misc = df['Misc'].dropna().astype(str).str.strip()\n",
    "    \n",
    "    # Combinar ambas series\n",
    "    combinados = pd.concat([elementos_X, elementos_Misc])\n",
    "    \n",
    "    # Eliminar entradas vacías después de stripping y convertir a lista de elementos únicos\n",
    "    combinados = combinados[combinados != '']\n",
    "    palabras_clave = combinados.unique().tolist()\n",
    "    \n",
    "    # Eliminar posibles duplicados adicionales y valores vacíos\n",
    "    palabras_clave = [elemento for elemento in palabras_clave if elemento]\n",
    "    \n",
    "    # Mostrar la lista 'palabras_clave'\n",
    "    print(f\"Lista 'palabras_clave' para la columna '{X}' y 'Misc':\\n\")\n",
    "    for idx, elemento in enumerate(palabras_clave, start=1):\n",
    "        print(f\"{idx}. {elemento}\")\n",
    "    \n",
    "    # (Opcional) Guardar la lista 'palabras_clave' en un archivo de texto\n",
    "    # Descomenta las siguientes líneas si deseas guardar los resultados\n",
    "    \"\"\"\n",
    "    nombre_archivo_salida = f'palabras_clave_{X.replace(\" \", \"_\")}_y_Misc.txt'\n",
    "    with open(nombre_archivo_salida, 'w', encoding='utf-8') as f:\n",
    "        for elemento in palabras_clave:\n",
    "            f.write(elemento + '\\n')\n",
    "    print(f\"\\nLa lista 'palabras_clave' ha sido guardada en el archivo '{nombre_archivo_salida}'.\")\n",
    "    \"\"\"\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{archivo_csv}' no se encontró en el directorio actual.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Error: El archivo '{archivo_csv}' está vacío.\")\n",
    "except pd.errors.ParserError:\n",
    "    print(f\"Error: El archivo '{archivo_csv}' no pudo ser parseado. Verifica el delimitador y el formato del archivo.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error de clave: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras clave normalizadas:\n",
      "['capacity building', 'civil society partnerships', 'communication technologies', 'debt sustainability', 'development assistance', 'disaggregated data', 'doha development agenda', 'entrepreneurship', 'environmentally sound technologies', 'foreign direct investments', 'fostering innovation', 'free trade', 'fundamental principles of official statistics', 'global partnership', 'global partnership for sustainable development', 'global stability', 'international aid', 'international cooperation', 'international population and housing census', 'international support', 'international support for developing countries', 'knowledge sharing', 'multistakeholder partnerships', 'poverty eradication', 'publicprivate partnerships', 'science cooperation agreements', 'technology cooperation agreements', 'technology transfer', 'weighted tariff average', 'women entrepreneurs', 'world trade organization', 'accountability', 'alternative energy', 'biodiversity', 'caring for country', 'co2 emissions', 'developing countries', 'disability', 'eco tourism', 'ecology', 'energy efficiency', 'environment', 'environmental', 'environmental degradation', 'environmental policy', 'environmental sustainability', 'equal rights to economic resources', 'ethical', 'foodenergywater nexus waterenergyfood nexus', 'forced displacement', 'good governance', 'governance', 'governance and policy', 'governance and risk', 'human rights', 'human wellbeing', 'inclusive', 'indigenous knowledge', 'intergenerational', 'irregular migration', 'kaitiakitanga', 'land locked developing countries', 'least developed countries', 'leave no one behind', 'low impact agriculture', 'low impact farming', 'low impact horticulture', 'migrant rights', 'migration and policy', 'policy coherence', 'pollution airsoilwater', 'promotion of shared responsibilities', 'recycling', 'refugee crisis', 'refugee rights', 'renewable', 'resilient', 'reuse technologies', 'small island developing states', 'smart cities', 'smart grid', 'smart houses', 'social protection policies', 'social responsibility', 'sustainability', 'sustainable', 'sustainable development', 'sustainable development goals', 'sustainable development indicators', 'sustainable management', 'sustainable public transport', 'sustainable societies', 'sustainable transport', 'technology for sustainable development', 'teleworking', 'transboundary cooperation', 'water sensitive revitalisation', 'wellbeing']\n",
      "\n",
      "Primer target normalizado:\n",
      "strengthen domestic resource mobilization including through international support to developing countries to improve domestic capacity for tax and other revenue collection total government revenue as a proportion of gdp by source proportion of domestic budget funded by domestic taxes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Normaliza el texto:\n",
    "    - Convierte a minúsculas\n",
    "    - Elimina acentos\n",
    "    - Elimina caracteres especiales\n",
    "    - Elimina espacios en blanco excesivos\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Eliminar acentos\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    # Eliminar caracteres especiales\n",
    "    texto = re.sub(r'[^a-z0-9\\s]', '', texto)\n",
    "    # Eliminar espacios en blanco excesivos\n",
    "    texto = ' '.join(texto.split())\n",
    "    return texto\n",
    "\n",
    "# Normalizar palabras clave\n",
    "palabras_clave_normalizadas = [normalizar_texto(palabra) for palabra in palabras_clave]\n",
    "\n",
    "# Normalizar targets\n",
    "targets_normalizados = [normalizar_texto(target) for target in targets]\n",
    "\n",
    "print(\"Palabras clave normalizadas:\")\n",
    "print(palabras_clave_normalizadas)\n",
    "print(\"\\nPrimer target normalizado:\")\n",
    "print(targets_normalizados[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo conceptos: 100%|██████████| 19/19 [00:00<00:00, 99.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conceptos extraídos para el primer target:\n",
      "{'domestic taxes', 'other revenue collection total government revenue', 'domestic budget', 'domestic capacity', 'international support', 'a proportion', 'developing countries', 'domestic resource mobilization', 'source proportion'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cargar el modelo de spaCy para inglés\n",
    "# Nota: Asegúrate de haber descargado el modelo 'en_core_web_sm' previamente\n",
    "# Puedes hacerlo ejecutando: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extraer_conceptos(target):\n",
    "    \"\"\"\n",
    "    Extrae conceptos clave de un target utilizando spaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(target)\n",
    "    conceptos = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Filtrar chunks demasiado cortos\n",
    "        if len(chunk.text.split()) < 2:\n",
    "            continue\n",
    "        # Filtrar chunks que sean solo stop words\n",
    "        if all(token.is_stop for token in chunk):\n",
    "            continue\n",
    "        # Añadir el chunk normalizado\n",
    "        conceptos.add(normalizar_texto(chunk.text))\n",
    "    return conceptos\n",
    "\n",
    "# Extraer conceptos de todos los targets\n",
    "targets_conceptos = []\n",
    "for target in tqdm(targets_normalizados, desc=\"Extrayendo conceptos\"):\n",
    "    conceptos = extraer_conceptos(target)\n",
    "    targets_conceptos.append(conceptos)\n",
    "\n",
    "# Ejemplo de conceptos extraídos para el primer target\n",
    "print(\"\\nConceptos extraídos para el primer target:\")\n",
    "print(targets_conceptos[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras clave expandidad con sinónimos restringidos:\n",
      "['least developed countries', 'environmental degradation', 'doha development agenda', 'inclusive', 'world trade organization', 'women entrepreneurs', 'science cooperation agreements', 'co2 emissions', 'ecology', 'low impact horticulture', 'fostering innovation', 'governance', 'developing countries', 'pollution airsoilwater', 'migration and policy', 'development assistance', 'sustainable management', 'energy efficiency', 'sustainability', 'refugee rights', 'land locked developing countries', 'low impact agriculture', 'technology for sustainable development', 'sustainable transport', 'small island developing states', 'capacity building', 'international aid', 'multistakeholder partnerships', 'sustainable societies', 'global stability', 'knowledge sharing', 'fundamental principles of official statistics', 'global partnership', 'foreign direct investments', 'international support', 'environmental sustainability', 'sustainable', 'smart grid', 'weighted tariff average', 'kaitiakitanga', 'leave no one behind', 'policy coherence', 'promotion of shared responsibilities', 'intergenerational', 'sustainable development', 'governance and risk', 'global partnership for sustainable development', 'smart houses', 'social protection policies', 'biodiversity', 'eco tourism', 'governance and policy', 'accountability', 'refugee crisis', 'sustainable development goals', 'sustainable public transport', 'entrepreneurship', 'disaggregated data', 'publicprivate partnerships', 'good governance', 'social responsibility', 'international support for developing countries', 'equal rights to economic resources', 'resilient', 'ethical', 'environmental policy', 'disability', 'technology transfer', 'foodenergywater nexus waterenergyfood nexus', 'environmentally sound technologies', 'caring for country', 'international cooperation', 'communication technologies', 'human wellbeing', 'renewable', 'human rights', 'civil society partnerships', 'indigenous knowledge', 'international population and housing census', 'free trade', 'poverty eradication', 'technology cooperation agreements', 'migrant rights', 'alternative energy', 'environment', 'teleworking', 'transboundary cooperation', 'environmental', 'forced displacement', 'irregular migration', 'sustainable development indicators', 'smart cities', 'recycling', 'low impact farming', 'wellbeing', 'water sensitive revitalisation', 'debt sustainability', 'reuse technologies']\n",
      "\n",
      "Todas las palabras clave expandidas están presentes en la lista original.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sgsr_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sgsr_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos de NLTK (si no se han descargado previamente)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def obtener_sinonimos(palabra):\n",
    "    \"\"\"\n",
    "    Obtiene sinónimos de una palabra en inglés utilizando WordNet.\n",
    "    \"\"\"\n",
    "    sinonimos = set()\n",
    "    for syn in wn.synsets(palabra, lang='eng'):\n",
    "        for lemma in syn.lemmas('eng'):\n",
    "            # Reemplazar guiones bajos con espacios y normalizar\n",
    "            sinonimos.add(normalizar_texto(lemma.name().replace('_', ' ')))\n",
    "    return list(sinonimos)\n",
    "\n",
    "# Función para obtener sinónimos restringidos a palabras_clave_normalizadas\n",
    "def obtener_sinonimos(palabra):\n",
    "    \"\"\"\n",
    "    Obtiene sinónimos de una palabra en inglés utilizando WordNet.\n",
    "    Solo devuelve sinónimos que ya están en palabras_clave_normalizadas.\n",
    "    \"\"\"\n",
    "    sinonimos = set()\n",
    "    for syn in wn.synsets(palabra, lang='eng'):\n",
    "        for lemma in syn.lemmas('eng'):\n",
    "            # Reemplazar guiones bajos con espacios y normalizar\n",
    "            sin = normalizar_texto(lemma.name().replace('_', ' '))\n",
    "            if sin in palabras_clave_normalizadas:\n",
    "                sinonimos.add(sin)\n",
    "    return list(sinonimos)\n",
    "\n",
    "# Expansión de palabras clave restringida a palabras_clave_normalizadas\n",
    "palabras_clave_expandidas = set(palabras_clave_normalizadas)\n",
    "for palabra in palabras_clave_normalizadas:\n",
    "    sinonimos_filtrados = obtener_sinonimos(palabra)\n",
    "    palabras_clave_expandidas.update(sinonimos_filtrados)\n",
    "\n",
    "palabras_clave_expandidas = list(palabras_clave_expandidas)\n",
    "\n",
    "print(\"\\nPalabras clave expandidad con sinónimos restringidos:\")\n",
    "print(palabras_clave_expandidas)\n",
    "\n",
    "# Verificar que todas las palabras clave expandidas estén en palabras_clave_normalizadas\n",
    "palabras_no_presentes = [p for p in palabras_clave_expandidas if p not in palabras_clave_normalizadas]\n",
    "if palabras_no_presentes:\n",
    "    print(\"\\nLas siguientes palabras no están en la lista original de palabras_clave:\")\n",
    "    print(palabras_no_presentes)\n",
    "else:\n",
    "    print(\"\\nTodas las palabras clave expandidas están presentes en la lista original.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de conceptos únicos: 139\n",
      "Total de palabras clave expandidas: 98\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo de embeddings multilingüe\n",
    "modelo = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Puedes elegir otro modelo si lo prefieres\n",
    "\n",
    "# Generar embeddings para palabras clave expandidas\n",
    "embeddings_palabras = modelo.encode(palabras_clave_expandidas, convert_to_tensor=True)\n",
    "\n",
    "# Generar embeddings para conceptos únicos\n",
    "# Primero, aplanamos la lista de conceptos\n",
    "conceptos_unicos = list(set(concepto for conceptos in targets_conceptos for concepto in conceptos))\n",
    "embeddings_conceptos = modelo.encode(conceptos_unicos, convert_to_tensor=True)\n",
    "\n",
    "print(f\"\\nTotal de conceptos únicos: {len(conceptos_unicos)}\")\n",
    "print(f\"Total de palabras clave expandidas: {len(palabras_clave_expandidas)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de mapeo de concepto a palabras clave:\n",
      "- least developed countries: ['least developed countries']\n",
      "- knowledge expertise technology: ['knowledge sharing']\n",
      "- housing census: ['international population and housing census']\n",
      "- all least developed countries: ['least developed countries']\n",
      "- developing countries proportion: ['developing countries']\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la matriz de similitud\n",
    "similitudes = util.pytorch_cos_sim(embeddings_conceptos, embeddings_palabras)\n",
    "\n",
    "# Definir umbral de similitud\n",
    "umbral_similitud = 0.7\n",
    "\n",
    "# Crear un diccionario para mapear conceptos a palabras clave\n",
    "mapa_concepto_palabra = {}\n",
    "\n",
    "for idx, concepto in enumerate(conceptos_unicos):\n",
    "    similitudes_concepto = similitudes[idx]\n",
    "    indices_similares = torch.where(similitudes_concepto >= umbral_similitud)[0]\n",
    "    palabras_similares = [palabras_clave_expandidas[i] for i in indices_similares]\n",
    "    if palabras_similares:\n",
    "        mapa_concepto_palabra[concepto] = palabras_similares\n",
    "\n",
    "print(\"\\nEjemplo de mapeo de concepto a palabras clave:\")\n",
    "for concepto, palabras in list(mapa_concepto_palabra.items())[:5]:\n",
    "    print(f\"- {concepto}: {palabras}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clusters de palabras clave similares:\n",
      "Cluster 3: ['science cooperation agreements', 'technology cooperation agreements']\n",
      "Cluster 4: ['governance', 'governance and risk', 'governance and policy', 'good governance']\n",
      "Cluster 14: ['migration and policy', 'irregular migration']\n",
      "Cluster 5: ['sustainable management', 'sustainability', 'technology for sustainable development', 'sustainable societies', 'environmental sustainability', 'sustainable', 'sustainable development', 'sustainable development goals', 'sustainable development indicators']\n",
      "Cluster 2: ['energy efficiency', 'alternative energy']\n",
      "Cluster 0: ['refugee rights', 'refugee crisis', 'migrant rights']\n",
      "Cluster 9: ['low impact agriculture', 'low impact farming']\n",
      "Cluster 10: ['sustainable transport', 'sustainable public transport']\n",
      "Cluster 6: ['multistakeholder partnerships', 'publicprivate partnerships']\n",
      "Cluster 1: ['global partnership', 'global partnership for sustainable development', 'international cooperation']\n",
      "Cluster 12: ['international support', 'international support for developing countries']\n",
      "Cluster 7: ['environmental policy', 'environmental']\n",
      "Cluster 25: ['human wellbeing', 'wellbeing']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Extraer los embeddings de las palabras clave\n",
    "embeddings_palabras_np = embeddings_palabras.cpu().numpy()\n",
    "\n",
    "# Aplicar Agglomerative Clustering con el métrico 'cosine'\n",
    "# Nota: 'cosine' métrico se utiliza directamente, no 'precomputed'\n",
    "# linkage='average' es compatible con 'cosine' métrico\n",
    "cluster = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    distance_threshold=0.3,  # Ajusta este umbral según tus necesidades\n",
    "    linkage='average',\n",
    "    metric='cosine'\n",
    "    #metric=None\n",
    ")\n",
    "\n",
    "clusters = cluster.fit_predict(embeddings_palabras_np)\n",
    "\n",
    "# Agrupar palabras clave por clusters\n",
    "from collections import defaultdict\n",
    "\n",
    "clusters_dict = defaultdict(list)\n",
    "for palabra, c in zip(palabras_clave_expandidas, clusters):\n",
    "    clusters_dict[c].append(palabra)\n",
    "\n",
    "print(\"\\nClusters de palabras clave similares:\")\n",
    "for c, palabras in clusters_dict.items():\n",
    "    if len(palabras) > 1:\n",
    "        print(f\"Cluster {c}: {palabras}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras clave representativas por cluster:\n",
      "Cluster 71: least developed countries\n",
      "Cluster 70: environmental degradation\n",
      "Cluster 41: doha development agenda\n",
      "Cluster 45: inclusive\n",
      "Cluster 58: world trade organization\n",
      "Cluster 69: women entrepreneurs\n",
      "Cluster 3: technology cooperation agreements\n",
      "Cluster 34: co2 emissions\n",
      "Cluster 73: ecology\n",
      "Cluster 67: low impact horticulture\n",
      "Cluster 43: fostering innovation\n",
      "Cluster 4: governance\n",
      "Cluster 50: developing countries\n",
      "Cluster 57: pollution airsoilwater\n",
      "Cluster 14: migration and policy\n",
      "Cluster 47: development assistance\n",
      "Cluster 5: sustainable development\n",
      "Cluster 2: energy efficiency\n",
      "Cluster 0: refugee rights\n",
      "Cluster 46: land locked developing countries\n",
      "Cluster 9: low impact farming\n",
      "Cluster 10: sustainable transport\n",
      "Cluster 53: small island developing states\n",
      "Cluster 37: capacity building\n",
      "Cluster 72: international aid\n",
      "Cluster 6: publicprivate partnerships\n",
      "Cluster 48: global stability\n",
      "Cluster 61: knowledge sharing\n",
      "Cluster 44: fundamental principles of official statistics\n",
      "Cluster 1: global partnership\n",
      "Cluster 38: foreign direct investments\n",
      "Cluster 12: international support\n",
      "Cluster 28: smart grid\n",
      "Cluster 51: weighted tariff average\n",
      "Cluster 63: kaitiakitanga\n",
      "Cluster 68: leave no one behind\n",
      "Cluster 56: policy coherence\n",
      "Cluster 52: promotion of shared responsibilities\n",
      "Cluster 55: intergenerational\n",
      "Cluster 33: smart houses\n",
      "Cluster 36: social protection policies\n",
      "Cluster 65: biodiversity\n",
      "Cluster 35: eco tourism\n",
      "Cluster 66: accountability\n",
      "Cluster 22: entrepreneurship\n",
      "Cluster 23: disaggregated data\n",
      "Cluster 39: social responsibility\n",
      "Cluster 60: equal rights to economic resources\n",
      "Cluster 40: resilient\n",
      "Cluster 49: ethical\n",
      "Cluster 7: environmental policy\n",
      "Cluster 24: disability\n",
      "Cluster 62: technology transfer\n",
      "Cluster 16: foodenergywater nexus waterenergyfood nexus\n",
      "Cluster 59: environmentally sound technologies\n",
      "Cluster 42: caring for country\n",
      "Cluster 30: communication technologies\n",
      "Cluster 25: human wellbeing\n",
      "Cluster 27: renewable\n",
      "Cluster 17: human rights\n",
      "Cluster 64: civil society partnerships\n",
      "Cluster 13: indigenous knowledge\n",
      "Cluster 32: international population and housing census\n",
      "Cluster 20: free trade\n",
      "Cluster 18: poverty eradication\n",
      "Cluster 31: environment\n",
      "Cluster 19: teleworking\n",
      "Cluster 11: transboundary cooperation\n",
      "Cluster 21: forced displacement\n",
      "Cluster 29: smart cities\n",
      "Cluster 54: recycling\n",
      "Cluster 15: water sensitive revitalisation\n",
      "Cluster 8: debt sustainability\n",
      "Cluster 26: reuse technologies\n"
     ]
    }
   ],
   "source": [
    "# Selección de representantes asegurando que pertenezcan a palabras_clave_normalizadas\n",
    "cluster_representante = {}\n",
    "for c, palabras in clusters_dict.items():\n",
    "    if len(palabras) == 1:\n",
    "        palabra = palabras[0]\n",
    "        if palabra in palabras_clave_normalizadas:\n",
    "            cluster_representante[c] = palabra\n",
    "        else:\n",
    "            # Opcional: no asignar representante o asignar una palabra válida\n",
    "            print(f\"Cluster {c} contiene una palabra no deseada: {palabra}. No se asigna representante.\")\n",
    "    else:\n",
    "        # Calcular similitud promedio de cada palabra con las demás en el cluster\n",
    "        indices = [palabras_clave_expandidas.index(p) for p in palabras]\n",
    "        similitudes_cluster = cosine_similarity(embeddings_palabras_np[indices])\n",
    "        sim_promedio = similitudes_cluster.mean(axis=1)\n",
    "        # Seleccionar la palabra con mayor similitud promedio\n",
    "        representante_idx = np.argmax(sim_promedio)\n",
    "        representante = palabras[representante_idx]\n",
    "        if representante in palabras_clave_normalizadas:\n",
    "            cluster_representante[c] = representante\n",
    "        else:\n",
    "            # Elegir la primera palabra que esté en palabras_clave_normalizadas\n",
    "            representante_alternativo = next((p for p in palabras if p in palabras_clave_normalizadas), None)\n",
    "            if representante_alternativo:\n",
    "                cluster_representante[c] = representante_alternativo\n",
    "            else:\n",
    "                print(f\"Cluster {c} no tiene representantes válidos en palabras_clave_normalizadas.\")\n",
    "\n",
    "print(\"\\nPalabras clave representativas por cluster:\")\n",
    "for c, rep in cluster_representante.items():\n",
    "    print(f\"Cluster {c}: {rep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de mapeo de concepto a palabras clave representativas:\n",
      "- least developed countries: ['least developed countries']\n",
      "- knowledge expertise technology: ['knowledge sharing']\n",
      "- housing census: ['international population and housing census']\n",
      "- all least developed countries: ['least developed countries']\n",
      "- developing countries proportion: ['developing countries']\n"
     ]
    }
   ],
   "source": [
    "# Mapeo de conceptos a palabras clave representativas\n",
    "mapa_concepto_palabra_representante = {}\n",
    "for concepto, palabras in mapa_concepto_palabra.items():\n",
    "    representantes = set()\n",
    "    for palabra in palabras:\n",
    "        # Encontrar el cluster de la palabra\n",
    "        cluster_id = next((c for c, p in clusters_dict.items() if palabra in p), None)\n",
    "        if cluster_id is not None:\n",
    "            representante = cluster_representante.get(cluster_id, None)\n",
    "            if representante and representante in palabras_clave_normalizadas:\n",
    "                representantes.add(representante)\n",
    "    if representantes:\n",
    "        mapa_concepto_palabra_representante[concepto] = list(representantes)\n",
    "\n",
    "print(\"\\nEjemplo de mapeo de concepto a palabras clave representativas:\")\n",
    "for concepto, palabras in list(mapa_concepto_palabra_representante.items())[:5]:\n",
    "    print(f\"- {concepto}: {palabras}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asignando palabras clave a targets: 100%|██████████| 19/19 [00:00<00:00, 18978.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de palabras clave asignadas a un target:\n",
      "goal_id                                                          17\n",
      "target_id                                                      17.1\n",
      "target            Strengthen domestic resource mobilization, inc...\n",
      "palabras_clave    [global partnership, international aid, intern...\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Asignación de palabras clave a cada target\n",
    "palabras_clave_por_target = []\n",
    "for conceptos in tqdm(targets_conceptos, desc=\"Asignando palabras clave a targets\"):\n",
    "    palabras_asignadas = set()\n",
    "    for concepto in conceptos:\n",
    "        palabras = mapa_concepto_palabra_representante.get(concepto, [])\n",
    "        palabras_asignadas.update(palabras)\n",
    "    palabras_clave_por_target.append(list(palabras_asignadas))\n",
    "\n",
    "# Crear el DataFrame con IDs desde SDG_final.csv\n",
    "targets_df = pd.DataFrame({\n",
    "    'goal_id': goal_ids,\n",
    "    'target_id': target_ids,\n",
    "    'target': targets,\n",
    "    'target_normalizado': targets_normalizados,\n",
    "    'conceptos': targets_conceptos,\n",
    "    'palabras_clave': palabras_clave_por_target\n",
    "})\n",
    "\n",
    "# Mostrar un ejemplo\n",
    "print(\"\\nEjemplo de palabras clave asignadas a un target:\")\n",
    "print(targets_df[['goal_id', 'target_id', 'target', 'palabras_clave']].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de targets sin palabras clave asignadas: 0\n",
      "Empty DataFrame\n",
      "Columns: [target, palabras_clave]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Identificar targets sin palabras clave asignadas\n",
    "targets_df['tiene_palabras_clave'] = targets_df['palabras_clave'].apply(lambda x: len(x) > 0)\n",
    "targets_sin_palabras = targets_df[targets_df['tiene_palabras_clave'] == False]\n",
    "\n",
    "print(f\"\\nTotal de targets sin palabras clave asignadas: {len(targets_sin_palabras)}\")\n",
    "print(targets_sin_palabras[['target', 'palabras_clave']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palabras clave más frecuentes: ['developing countries', 'international support', 'global partnership']\n",
      "\n",
      "Total de targets sin palabras clave asignadas después de la asignación de palabras frecuentes: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calcular la frecuencia de cada palabra clave\n",
    "contador_palabras = Counter([palabra for sublist in palabras_clave_por_target for palabra in sublist])\n",
    "palabras_frecuentes = [palabra for palabra, _ in contador_palabras.most_common(3)]  # Top 3 palabras clave\n",
    "\n",
    "print(\"\\nPalabras clave más frecuentes:\", palabras_frecuentes)\n",
    "\n",
    "# Asignar las top 3 palabras clave más frecuentes a targets sin palabras clave\n",
    "for i, row in targets_df.iterrows():\n",
    "    if not row['tiene_palabras_clave']:\n",
    "        targets_df.at[i, 'palabras_clave'] = palabras_frecuentes\n",
    "\n",
    "# Verificar nuevamente\n",
    "print(f\"\\nTotal de targets sin palabras clave asignadas después de la asignación de palabras frecuentes: {len(targets_df[targets_df['palabras_clave'].apply(lambda x: len(x) == 0)])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivo CSV generado exitosamente: 'targets_con_palabras_clave_agrupadas.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Concatenar las palabras clave separadas por comas\n",
    "targets_df['palabras_clave_concatenadas'] = targets_df['palabras_clave'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Seleccionar las columnas deseadas, incluyendo goal_id y target_id\n",
    "resultado_csv = targets_df[['goal_id', 'target_id', 'target', 'palabras_clave_concatenadas']]\n",
    "\n",
    "# Definir la ruta del archivo CSV final\n",
    "file_path = 'targets_con_palabras_clave_agrupadas.csv'\n",
    "\n",
    "# Verificar si el archivo ya existe\n",
    "file_exists = os.path.isfile(file_path)\n",
    "# Concatenar las palabras clave separadas por comas\n",
    "targets_df['palabras_clave_concatenadas'] = targets_df['palabras_clave'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "# Seleccionar las columnas deseadas, incluyendo goal_id y target_id\n",
    "resultado_csv = targets_df[['goal_id', 'target_id', 'target', 'palabras_clave_concatenadas']]\n",
    "\n",
    "# Definir la ruta del archivo CSV final\n",
    "file_path = 'targets_con_palabras_clave_agrupadas.csv'\n",
    "\n",
    "# Verificar si el archivo ya existe\n",
    "file_exists = os.path.isfile(file_path)\n",
    "\n",
    "# Si el archivo ya existe, leerlo para evitar duplicados\n",
    "if file_exists:\n",
    "    final_df = pd.read_csv(file_path)\n",
    "    # Filtrar los targets que ya están en el archivo final\n",
    "    nuevos_resultados = resultado_csv[~resultado_csv['target_id'].isin(final_df['target_id'])]\n",
    "else:\n",
    "    nuevos_resultados = resultado_csv\n",
    "\n",
    "# Guardar los nuevos resultados en el archivo CSV usando append si ya existe\n",
    "nuevos_resultados.to_csv(file_path, mode='a', index=False, header=not file_exists)\n",
    "\n",
    "print(f\"\\nArchivo CSV generado exitosamente: '{file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p_mo412a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
