https://api.elsevier.com/content/article/pii/B9780443186448000058 doi:10.1016/B978-0-443-18644-8.00005-8 3-s2.0-B9780443186448000058 10.1016/B978-0-443-18644-8.00005-8 B978-0-443-18644-8.00005-8 Chapter 21 Deep learning applied solid waste recognition system targeting sustainable development goal  Machine Intelligence in Mechanical Engineering EBook chp 9780443186448 393 421 393-421 urn:isbn:978-0-443-18644-8 text/plain 2024-12-31 2024 Copyright © 2024 Elsevier Inc. All rights reserved. Elsevier Inc. Lee, Kok Jin Chiong, Meng-Choung Hassan, Cik Suhana Natarajan, Elango Solihin, Mahmud Iwan Lim, Wei Hong 
               In Malaysia, it was estimated that approximately 250,000 tons of municipal solid garbage were produced every day because of our daily activities. Categorizing these municipal solid trashes efficiently into respective types for recycling purposes has been imminent to minimize their adverse effects on our environment. This chapter presented a solid waste sorting system to distinguish three different types of solid waste by using a deep learning method. The YOLOv3 was proposed as a real-time object identification module to differentiate solid trash. YOLOv3 and YOLOv3-tiny were trained to recognize plastic bottles, paper, and metal with 150 distinct photos. When compared to the YOLOv3-tiny model, the YOLOv3 model produced a notably higher mean average precision (mAP). Due to the greater numbers of the convolutional layer in the latter, the mAP for YOLOv3 was averaging 81.93%, whereas the mAP for YOLOv3 was only 74.58%. Furthermore, the YOLOv3 convergence rate was also substantially faster than that of YOLOv3-tiny. YOLOv3 object detection accuracy for plastic bottles, paper, and metal is 0.98, 1, and 1, respectively, under static conditions. YOLOv3-tiny, on the other hand, was unable to detect plastic bottles, with detection accuracies of just 0.68 and 0.94 for paper and metal cans, respectively. Such attributes feature YOLOv3 as a viable object recognition tool for use in a solid waste sorting system.
             0 false  false    Solid waste management solid waste deep learning waste segregation and convolutional neural network    https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si1.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si10.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si11.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si12.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si13.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si14.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si15.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si16.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si17.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si18.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si19.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si2.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si20.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si21.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si22.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si23.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si24.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si3.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si4.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si5.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si6.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si7.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si8.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-si9.svg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-01-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-01-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-02-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-02-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-03-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-03-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-04-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-04-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-05-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-05-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-06-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-06-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-07-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-07-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-08-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-08-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-09-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-09-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-10-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-10-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-11-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-11-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-12-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-12-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-13-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-13-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-14-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-14-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-15-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-15-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-16-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-16-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-17-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-17-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-18-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-18-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-19-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-19-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-20-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-20-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-21-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-21-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-22-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-22-9780443186448.sml?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-23-9780443186448.jpg?httpAccept=%2A%2F%2A https://api.elsevier.com/content/object/eid/3-s2.0-B9780443186448000058-f21-23-9780443186448.sml?httpAccept=%2A%2F%2A  85191134348 2-s2.0-85191134348  nonserial BK 785094 291210 291811 35  Machine Intelligence in Mechanical Engineering MACHINEINTELLIGENCEINMECHANICALENGINEERING 2024-01-17 2024-01-25 2024-01-17 2024-01-25 2024-01-25T07:23:44 3-s2.0-B9780443186448000058 B978-0-443-18644-8.00005-8 B9780443186448000058 10.1016/B978-0-443-18644-8.00005-8 S300 S300.1 FULL-TEXT 3-s2.0-C2022000430X 10.1016/C2022-0-00430-X 2024-01-25T07:56:31.424344Z 0 0 20240101 20241231 2024 2024-01-17T18:14:01.980271Z articleinfo articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids confeditor contenttype copyright dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate isbn isbns isbnnorm isbnsnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pglast pii piinorm pubdateend pubdatestart pubdatetxt pubyr seriesname sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings tomb webpdf webpdfpagecount yearnav affil articletitle auth authfirstini authfull authkeywords authlast primabst ref figure table body mmlmath 978-0-443-18644-8 9780443186448 978-0-443-18645-5 9780443186455  Woodhead Publishing Reviews: Mechanical Engineering Series  Machine Intelligence in Mechanical Engineering 26 393 421 393 421  2024 2024-01-01 2024-12-31 2024 K. Palanikumar  Department of Mechanical Engineering, Sri Sairam Institute of Technology, Chennai, Tamil Nadu, India Department of Mechanical Engineering, Sri Sairam Institute of Technology Chennai Tamil Nadu India  Department of Mechanical Engineering, Sri Sairam Institute of Technology, Chennai, Tamil Nadu, India   Elango Natarajan  Department of Mechanical Engineering, Faculty of Engineering, Technology and Built Environment, UCSI University, Kuala Lumpur, Malaysia Department of Mechanical Engineering, Faculty of Engineering, Technology and Built Environment, UCSI University Kuala Lumpur Malaysia  Department of Mechanical Engineering, Faculty of Engineering, Technology and Built Environment, UCSI University, Kuala Lumpur, Malaysia   S. Ramesh  Department of Mechanical Engineering, Jerusalem College of Engineering, Chennai, Tamil Nadu, India Department of Mechanical Engineering, Jerusalem College of Engineering Chennai Tamil Nadu India  Department of Mechanical Engineering, Jerusalem College of Engineering, Chennai, Tamil Nadu, India   J. Paulo Davim  Department of Mechanical Engineering, University of Aveiro, Aveiro, Portugal Department of Mechanical Engineering, University of Aveiro Aveiro Portugal  Department of Mechanical Engineering, University of Aveiro, Aveiro, Portugal      chapter chp Copyright © 2024 Elsevier Inc. All rights reserved. DEEPLEARNINGAPPLIEDSOLIDWASTERECOGNITIONSYSTEMTARGETINGSUSTAINABLEDEVELOPMENTGOAL LEE K 21.1 Introduction  21.2 Methods 21.2.1 Conceptual design of a waste segregation machine  21.2.2 Comparison of the object detection module  21.2.3 Training YOLOv3   21.3 Results and discussions 21.3.1 Evaluation of Training  21.3.2 Training results for classes  21.3.3 Training loss result  21.3.4 Testing image and prototype   21.4 Conclusion  References   AJA 2014 693 710 O  VONGDALA 2018 N  DAS 2019 658 678 S   MA 2016 589 594 H  JEREME 2015 1 22 I  SHEKDAR 2009 1438 1448 A  PARVEZ 2019 20 28 N  FERRONATO 2019 N  GUNDUPALLI 2017 56 74 S   ADEDEJI 2019 607 612 O  GUTBERLET 2017 299 310 J  NOWAKOWSKI 2020 1 9 P  ELEONU 2020 193 203 F  MOHSEN 2018 68 71 H  CHEN 2020 106303 H  WANG 2014 34003 H  MAEDAGUTIERREZ 2020 V  KUMAR 2021 S  XIAO 2020 Y  YANG 2021 1837 1844 H  XIAO 2021 18 25 J  REDMON 2018 J   LIU 2020 12150 J   JU 2019 M  KUMAR 2019 12062 B  LI 2019 J       HE 2019 W   PARK 2020 M  ZHANG 2019 32068 M   LEEX2024X393 LEEX2024X393X421 LEEX2024X393XK LEEX2024X393X421XK    https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004  item B978-0-443-18644-8.00005-8 B9780443186448000058 3-s2.0-B9780443186448000058 10.1016/B978-0-443-18644-8.00005-8 785094 2024-01-25T07:56:31.424344Z 2024-01-01 2024-12-31 3-s2.0-B9780443186448000058-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/MAIN/application/pdf/b707f7da2cb358bb3a652a85bf4a2051/main.pdf main.pdf pdf true 3587650 MAIN 29 3-s2.0-B9780443186448000058-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/PREVIEW/image/png/4a6988c26cfcdebf6b8ffea485a833bd/main_1.png main_1.png png 57677 849 656 IMAGE-WEB-PDF 1    3-s2.0-B9780443186448000058-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/3e2418f827b4236e12c8b4604322cb82/si1.svg si1 si1.svg svg 3691 ALTIMG  3-s2.0-B9780443186448000058-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/e3f5675c0794138452e1900ed6ebb0f5/si10.svg si10 si10.svg svg 11130 ALTIMG  3-s2.0-B9780443186448000058-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/3409becadc6a9437d7c1a04fb28e85f2/si11.svg si11 si11.svg svg 10284 ALTIMG  3-s2.0-B9780443186448000058-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/7b616ce699ec8fda912296c52012b01a/si12.svg si12 si12.svg svg 27783 ALTIMG  3-s2.0-B9780443186448000058-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/ab904ea369b929880691e130d3614a9e/si13.svg si13 si13.svg svg 3652 ALTIMG  3-s2.0-B9780443186448000058-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/1b1c3d6472982a45a146ee66941b665a/si14.svg si14 si14.svg svg 3312 ALTIMG  3-s2.0-B9780443186448000058-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/c0c21e6f70233389a9e62180a8cf67a6/si15.svg si15 si15.svg svg 62255 ALTIMG  3-s2.0-B9780443186448000058-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/5aceb817a1c670073e379c16188cd7e3/si16.svg si16 si16.svg svg 10326 ALTIMG  3-s2.0-B9780443186448000058-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/1ab30ac1ae80a008acfb877ff337615f/si17.svg si17 si17.svg svg 8963 ALTIMG  3-s2.0-B9780443186448000058-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/ca817b9bdbd39830a72dbb6b2b6eb6c2/si18.svg si18 si18.svg svg 5301 ALTIMG  3-s2.0-B9780443186448000058-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/dccdb607fc79f52241fcf579846be098/si19.svg si19 si19.svg svg 39863 ALTIMG  3-s2.0-B9780443186448000058-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/83a734a38004fef26b649da9051d9a8d/si2.svg si2 si2.svg svg 3655 ALTIMG  3-s2.0-B9780443186448000058-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/ab4e77a75a36507a5bbf161f51b5ea1e/si20.svg si20 si20.svg svg 4837 ALTIMG  3-s2.0-B9780443186448000058-si21.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/6a71851b6aaf9fac5b909a259fa35791/si21.svg si21 si21.svg svg 3455 ALTIMG  3-s2.0-B9780443186448000058-si22.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/9c5fbb3844c00f1914c8b97a57d551bd/si22.svg si22 si22.svg svg 3116 ALTIMG  3-s2.0-B9780443186448000058-si23.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/b75add4c84a862508bbdb9fa359d8b99/si23.svg si23 si23.svg svg 33792 ALTIMG  3-s2.0-B9780443186448000058-si24.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/21171f74dc8acadb93162c03f1eed8e8/si24.svg si24 si24.svg svg 24635 ALTIMG  3-s2.0-B9780443186448000058-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/b295eeaa599a906973c94d51453eb2f3/si3.svg si3 si3.svg svg 3546 ALTIMG  3-s2.0-B9780443186448000058-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/39ec4f37481fbd716e0ccf0300cb5ab6/si4.svg si4 si4.svg svg 3691 ALTIMG  3-s2.0-B9780443186448000058-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/a76de5d6d1f288d6e182f85394f9ce20/si5.svg si5 si5.svg svg 3655 ALTIMG  3-s2.0-B9780443186448000058-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/3b3d190e6eef27bfcb7cc12322b7b89a/si6.svg si6 si6.svg svg 20053 ALTIMG  3-s2.0-B9780443186448000058-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/02708e59484b94b9bce4ae6fd6a23711/si7.svg si7 si7.svg svg 24024 ALTIMG  3-s2.0-B9780443186448000058-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/8f6f781bf2d6e441e143195db28c11c0/si8.svg si8 si8.svg svg 9422 ALTIMG  3-s2.0-B9780443186448000058-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/STRIPIN/image/svg+xml/bfd2d8c8a7257163b0ea5eb2be23368c/si9.svg si9 si9.svg svg 12782 ALTIMG  3-s2.0-B9780443186448000058-f21-01-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-01-9780443186448/DOWNSAMPLED/image/jpeg/744bec2d10e2820c8045e769653e7d21/f21-01-9780443186448.jpg f21-01-9780443186448 f21-01-9780443186448.jpg jpg 12158 183 339 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-01-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-01-9780443186448/THUMBNAIL/image/gif/82f9359dd6412c12fd93269c28a5188b/f21-01-9780443186448.sml f21-01-9780443186448 f21-01-9780443186448.sml sml 4819 118 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-02-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-02-9780443186448/DOWNSAMPLED/image/jpeg/3b9e9fba65a103e16790097fdc328b5c/f21-02-9780443186448.jpg f21-02-9780443186448 f21-02-9780443186448.jpg jpg 17410 170 434 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-02-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-02-9780443186448/THUMBNAIL/image/gif/97b15d842add5246242d92fa8cf67bc1/f21-02-9780443186448.sml f21-02-9780443186448 f21-02-9780443186448.sml sml 2509 86 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-03-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-03-9780443186448/DOWNSAMPLED/image/jpeg/e4d1f4cebf3328c984c1338fb56d742e/f21-03-9780443186448.jpg f21-03-9780443186448 f21-03-9780443186448.jpg jpg 17121 165 499 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-03-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-03-9780443186448/THUMBNAIL/image/gif/fc0a1ffc9d05d4365a9d9e7d55ea97a2/f21-03-9780443186448.sml f21-03-9780443186448 f21-03-9780443186448.sml sml 3056 72 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-04-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-04-9780443186448/DOWNSAMPLED/image/jpeg/07837289ad329c8453b8f23a1cc4192e/f21-04-9780443186448.jpg f21-04-9780443186448 f21-04-9780443186448.jpg jpg 20800 271 499 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-04-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-04-9780443186448/THUMBNAIL/image/gif/1fd3c58cab27845ba125cc4045c4a6d2/f21-04-9780443186448.sml f21-04-9780443186448 f21-04-9780443186448.sml sml 4244 119 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-05-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-05-9780443186448/DOWNSAMPLED/image/jpeg/38e7a24e35885c6471d8589ceb238041/f21-05-9780443186448.jpg f21-05-9780443186448 f21-05-9780443186448.jpg jpg 27871 598 499 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-05-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-05-9780443186448/THUMBNAIL/image/gif/bfada4f077bb7cd50bff0a7b3bc71a19/f21-05-9780443186448.sml f21-05-9780443186448 f21-05-9780443186448.sml sml 3140 164 137 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-06-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-06-9780443186448/DOWNSAMPLED/image/jpeg/0b29c4ed36a4293be4fe366112b49d73/f21-06-9780443186448.jpg f21-06-9780443186448 f21-06-9780443186448.jpg jpg 42452 409 452 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-06-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-06-9780443186448/THUMBNAIL/image/gif/b8765a0f71c4d951a8a7b1e0ed159353/f21-06-9780443186448.sml f21-06-9780443186448 f21-06-9780443186448.sml sml 13063 164 181 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-07-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-07-9780443186448/DOWNSAMPLED/image/jpeg/0314deb6ccc35b65bdf0144304df8ede/f21-07-9780443186448.jpg f21-07-9780443186448 f21-07-9780443186448.jpg jpg 19093 187 499 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-07-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-07-9780443186448/THUMBNAIL/image/gif/cde87e324ae101af6ab3198b5c821f85/f21-07-9780443186448.sml f21-07-9780443186448 f21-07-9780443186448.sml sml 4277 82 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-08-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-08-9780443186448/DOWNSAMPLED/image/jpeg/46ee9ab1ac34f912709f1aba440f048a/f21-08-9780443186448.jpg f21-08-9780443186448 f21-08-9780443186448.jpg jpg 37454 367 263 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-08-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-08-9780443186448/THUMBNAIL/image/gif/fa10deea1174db92834602c6915fc04f/f21-08-9780443186448.sml f21-08-9780443186448 f21-08-9780443186448.sml sml 8893 163 117 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-09-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-09-9780443186448/DOWNSAMPLED/image/jpeg/f9440c3a64d929ace5644965153badc4/f21-09-9780443186448.jpg f21-09-9780443186448 f21-09-9780443186448.jpg jpg 9290 284 263 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-09-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-09-9780443186448/THUMBNAIL/image/gif/ff79300f626542ad00f8738d9b8a62e0/f21-09-9780443186448.sml f21-09-9780443186448 f21-09-9780443186448.sml sml 3230 164 152 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-10-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-10-9780443186448/DOWNSAMPLED/image/jpeg/f1daa275fe63749b47cb638e54a45774/f21-10-9780443186448.jpg f21-10-9780443186448 f21-10-9780443186448.jpg jpg 11498 339 263 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-10-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-10-9780443186448/THUMBNAIL/image/gif/98dca24316ab9a926c9fa760e87675bd/f21-10-9780443186448.sml f21-10-9780443186448 f21-10-9780443186448.sml sml 3175 164 127 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-11-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-11-9780443186448/DOWNSAMPLED/image/jpeg/6b4d3f1367b509f34695a226fe6c9173/f21-11-9780443186448.jpg f21-11-9780443186448 f21-11-9780443186448.jpg jpg 38653 285 376 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-11-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-11-9780443186448/THUMBNAIL/image/gif/096cf5f783aa4d818a890428e1d33664/f21-11-9780443186448.sml f21-11-9780443186448 f21-11-9780443186448.sml sml 29728 164 216 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-12-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-12-9780443186448/DOWNSAMPLED/image/jpeg/a0cc9eddabffc5a649997aa0b3935b8a/f21-12-9780443186448.jpg f21-12-9780443186448 f21-12-9780443186448.jpg jpg 21605 89 301 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-12-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-12-9780443186448/THUMBNAIL/image/gif/c1a9dea400a43113f287f7b17ec7c435/f21-12-9780443186448.sml f21-12-9780443186448 f21-12-9780443186448.sml sml 7420 65 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-13-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-13-9780443186448/DOWNSAMPLED/image/jpeg/e33305e6d7604121147fe84d6eb005ef/f21-13-9780443186448.jpg f21-13-9780443186448 f21-13-9780443186448.jpg jpg 17268 332 452 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-13-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-13-9780443186448/THUMBNAIL/image/gif/3d0f355aa7f5139abc7814845a3bc926/f21-13-9780443186448.sml f21-13-9780443186448 f21-13-9780443186448.sml sml 4573 161 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-14-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-14-9780443186448/DOWNSAMPLED/image/jpeg/99805cc9e706feca47319384ecf2c4d0/f21-14-9780443186448.jpg f21-14-9780443186448 f21-14-9780443186448.jpg jpg 21684 336 452 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-14-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-14-9780443186448/THUMBNAIL/image/gif/c11240a19ceb967e447174934520d212/f21-14-9780443186448.sml f21-14-9780443186448 f21-14-9780443186448.sml sml 5663 163 219 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-15-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-15-9780443186448/DOWNSAMPLED/image/jpeg/f712ccaf09bf0745bf95537f79546d5e/f21-15-9780443186448.jpg f21-15-9780443186448 f21-15-9780443186448.jpg jpg 77994 497 499 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-15-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-15-9780443186448/THUMBNAIL/image/gif/8d85f401eb75b29c34736d381f32b3a9/f21-15-9780443186448.sml f21-15-9780443186448 f21-15-9780443186448.sml sml 9175 163 164 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-16-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-16-9780443186448/DOWNSAMPLED/image/jpeg/e7e11379633983bce99242d07ffef5c0/f21-16-9780443186448.jpg f21-16-9780443186448 f21-16-9780443186448.jpg jpg 84563 496 499 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-16-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-16-9780443186448/THUMBNAIL/image/gif/1be8d96c25d7abaf76ef1f19ed29c8ce/f21-16-9780443186448.sml f21-16-9780443186448 f21-16-9780443186448.sml sml 12179 164 165 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-17-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-17-9780443186448/DOWNSAMPLED/image/jpeg/a6b46be7e807b0fc219b0a99ffd96595/f21-17-9780443186448.jpg f21-17-9780443186448 f21-17-9780443186448.jpg jpg 33409 289 376 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-17-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-17-9780443186448/THUMBNAIL/image/gif/b75907aa8b08bde4e2ce84014f14234e/f21-17-9780443186448.sml f21-17-9780443186448 f21-17-9780443186448.sml sml 16192 164 213 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-18-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-18-9780443186448/DOWNSAMPLED/image/jpeg/e3b245584a442c5fe40dba7caba50889/f21-18-9780443186448.jpg f21-18-9780443186448 f21-18-9780443186448.jpg jpg 22603 278 339 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-18-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-18-9780443186448/THUMBNAIL/image/gif/d88366fe139a0089fbbb223e76776b4e/f21-18-9780443186448.sml f21-18-9780443186448 f21-18-9780443186448.sml sml 18020 164 200 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-19-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-19-9780443186448/DOWNSAMPLED/image/jpeg/ecfb328a368a99e2a6d9578c9d9cee1e/f21-19-9780443186448.jpg f21-19-9780443186448 f21-19-9780443186448.jpg jpg 25468 332 376 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-19-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-19-9780443186448/THUMBNAIL/image/gif/64d56c32c32bd63dddfdfcbfa3a471c6/f21-19-9780443186448.sml f21-19-9780443186448 f21-19-9780443186448.sml sml 13806 164 186 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-20-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-20-9780443186448/DOWNSAMPLED/image/jpeg/924e9790a1811e8dcd9c0ef3a9183d55/f21-20-9780443186448.jpg f21-20-9780443186448 f21-20-9780443186448.jpg jpg 31208 334 376 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-20-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-20-9780443186448/THUMBNAIL/image/gif/93013613d72765c78744e4368f037349/f21-20-9780443186448.sml f21-20-9780443186448 f21-20-9780443186448.sml sml 16755 163 184 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-21-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-21-9780443186448/DOWNSAMPLED/image/jpeg/d12396bba795b820fbdd59fee52f91fb/f21-21-9780443186448.jpg f21-21-9780443186448 f21-21-9780443186448.jpg jpg 65730 429 376 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-21-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-21-9780443186448/THUMBNAIL/image/gif/574da63a284ea53cb99bcc864f83e43a/f21-21-9780443186448.sml f21-21-9780443186448 f21-21-9780443186448.sml sml 19988 163 143 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-22-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-22-9780443186448/DOWNSAMPLED/image/jpeg/09bd55b806c293d16c39494b9594f727/f21-22-9780443186448.jpg f21-22-9780443186448 f21-22-9780443186448.jpg jpg 56929 372 376 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-22-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-22-9780443186448/THUMBNAIL/image/gif/afd807d4c94426499ca8f5ee7271bd69/f21-22-9780443186448.sml f21-22-9780443186448 f21-22-9780443186448.sml sml 22283 163 165 IMAGE-THUMBNAIL  3-s2.0-B9780443186448000058-f21-23-9780443186448.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-23-9780443186448/DOWNSAMPLED/image/jpeg/64a77199cffdce4f6c59f20469d14338/f21-23-9780443186448.jpg f21-23-9780443186448 f21-23-9780443186448.jpg jpg 40925 331 414 IMAGE-DOWNSAMPLED  3-s2.0-B9780443186448000058-f21-23-9780443186448.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:B9780443186448000058/f21-23-9780443186448/THUMBNAIL/image/gif/08f6a174d514185d8b0cc4b645fa2ba8/f21-23-9780443186448.sml f21-23-9780443186448 f21-23-9780443186448.sml sml 21607 164 205 IMAGE-THUMBNAIL     B978-0-443-18644-8.00005-8 10.1016/B978-0-443-18644-8.00005-8 978-0-443-18644-8 Elsevier Inc.  Figure 21.1 Composite of municipal solid waste.    Figure 21.2 Flow of solid waste management [3,7,8] .    Figure 21.3 Architecture of convolutional neural network [18,19] .    Figure 21.4 Conceptual design of a solid waste sorting machine.    Figure 21.5 Flowchart of the solid waste sorting machine operation.    Figure 21.6 Computer-aided drawing for the waste sorting machine operation.    Figure 21.7 Architecture of YOLOv3 [20,22,28] .    Figure 21.8 Layout of DarkNet-53.  Adapted from J. Redmon, A. Farhadi, YOLOv3: an incremental improvement. Comput. Sci. (2018) [24] .   Figure 21.9 Anchor box of YOLOv3 [22,30] .    Figure 21.10 Diagram for calculation of IoU) [31], [32], [33] .    Figure 21.11 Using LabelImg for labeling objects in an image.    Figure 21.12 Example output of txt file after using LabelImg.    Figure 21.13 Loss graph of YOLOv3.    Figure 21.14 Loss graph of YOLOv3-tiny.    Figure 21.15 mAP and loss graph of YOLOv3.    Figure 21.16 mAP and loss graph of YOLOv3-tiny.    Figure 21.17 Testing image of plastic to YOLOv3.    Figure 21.18 Testing image of plastic to YOLOv3-tiny.    Figure 21.19 Testing image of paper to YOLOv3.    Figure 21.20 Testing image of paper to YOLOv3-tiny.    Figure 21.21 Testing image of metal to YOLOv3.    Figure 21.22 Testing image of metal to YOLOv3-tiny.    Figure 21.23 Metal can detection by YOLOv3 when it is transferred by conveyor belt.    Table 21.1 Technique of direct sorting [10] .    Name Description   Screw press Separating MSW by squeezing the organic waste through slits.  Disk Screen Separate the MSW by size and weight.  Shredder with a magnet Separate the organic waste and paper from MSW using a shredder and magnet.  Magnetic drum Using the magnetic field from the magnetic drum to remove the ferromagnetic waste.  Magnetic density separation Separate the polymeric wastes by their density and floating level in the magnetic liquid.  Triboelectricities separation Using the contact electrification and frictional electrification theory to separate the plastic pieces by creating a charge on those pieces.     Table 21.2 Technique of indirect sorting [10] .    Name Description   Eddy current with sensor and air jet Using the electromagnetic sensor to detect the material in the eddy current method. If the sensor detected the ferrous metal, the air jet will be triggered.  Laser-induced breakdown spectroscopy Using the laser pulse to detect and analyze materials like plastic, metal, and wood. It usually came with the spectrometer, Nd:YAG, and processing unit to analyze the data. The air jet which is controlled by the processing unit will be used to separate those materials into different bins.  X-ray The X-ray will penetrate the material and reach the radiation detector. The radiation detector will analyze the info and transmit it to the processing unit to control the air jet to separate the material.  Optical-based sorting Using the camera to identify the material based on solid waste’s color. It can be combined with other sensors like a D-shape detection camera, inductive sensor, weight sensor, and so on.  Spectral imaging-based sorting It is the method that implements the image processing method to analyze solid waste. Techniques like near-infrared, visual image spectroscopy, and hyperspectral imaging are implemented in this method.     Table 21.3 Training results.            Iteration Model F1-score Precision, Recall N  TP     N  FP     N  FN     %Average IoU %mAP   1000 V3 0.63 0.72 0.55 163 62 130 50.82 61.28  V3 tiny 0 0 0 0 0 292 0 0.01  2000 V3 0.83 0.86 0.81 236 40 56 66.90 83.09  V3 tiny 0.12 0.36 0.07 20 35 272 22.59 6.93  3000 V3 0.82 0.85 0.80.07 235 43 57 69.79 83.62  V3 tiny 0.27 0.88 0.41 78 11 214 58.51 48.49  4000 V3 0.84 0.91 0.79 230 24 62 74.28 82.52  V3 tiny 0.63 0.79 0.52 151 39 141 54.74 57.80  5000 V3 0.83 0.87 0.79 231 34 61 75.24 82.10  V3 tiny 0.78 0.90 0.68 199 22 93 69.66 74.45  6000 V3 0.83 0.87 0.80 233 35 59 74.90 81.93   V3 tiny 0.75 0.86 0.67 195 31 97 67.99 74.58     Table 21.4 Training results for classes.        Iteration Model Class AP N  TP     N  FP       1000 V3 Plastic 70.21 59 18  Paper 53.62 36 16  Metal 60.00 67 28  V3 tiny Plastic 2.25 0 0  Paper 0.65 0 0  Metal 1.55 0 0  2000 V3 Plastic 92.57 95 9  Paper 77.81 58 18  Metal 80.48 82 16  V3 tiny Plastic 2.59 0 0  Paper 2.08 0 17  Metal 16.12 20 18  3000 V3 Plastic 92.57 95 9  Paper 77.81 58 18  Metal 80.48 82 16  V3 tiny Plastic 45.30 32 9  Paper 40.95 12 0  Metal 59.22 34 2  4000 V3 Plastic 88.85 91 8  Paper 80.07 58 6  Metal 78.66 81 10  V3 tiny Plastic 63.30 57 15  Paper 48.14 35 19  Metal 61.95 59 5  5000 V3 Plastic 91.62 95 12  Paper 73.99 55 14  Metal 80.67 81 8  V3 tiny Plastic 85.08 81 10  Paper 68.98 49 9  Metal 72.31 69 3  6000 V3 Plastic 90.78 96 11    Paper 74.71 56 16    Metal 80.31 81 8   V3 tiny Plastic 83.99 81 16    Paper 68.95 50 9    Metal 70.80 64 6     Table 21.5 Training loss result.     Iteration Model Average loss   1000 V3 0.863667  V3 tiny 2.481995  2000 V3 0.335003  V3 tiny 2.302766  3000 V3 0.250272  V3 tiny 1.507307  4000 V3 0.186384  V3 tiny 1.253516  5000 V3 0.113712  V3 tiny 1.043454  6000 V3 0.123053   V3 tiny 0.964706      Chapter 21 Deep learning applied solid waste recognition system targeting sustainable development goal Kok Jin Lee  Meng-Choung Chiong  Cik Suhana Hassan  Elango Natarajan  Mahmud Iwan Solihin  Wei Hong Lim  Department of Mechanical Engineering, Faculty of Engineering, Technology and Built Environment, UCSI University, Kuala Lumpur, Malaysia Department of Mechanical Engineering, Faculty of Engineering, Technology and Built Environment, UCSI University Kuala Lumpur Malaysia  Department of Mechanical Engineering, Faculty of Engineering, Technology and Built Environment, UCSI University, Kuala Lumpur, Malaysia   In Malaysia, it was estimated that approximately 250,000 tons of municipal solid garbage were produced every day because of our daily activities. Categorizing these municipal solid trashes efficiently into respective types for recycling purposes has been imminent to minimize their adverse effects on our environment. This chapter presented a solid waste sorting system to distinguish three different types of solid waste by using a deep learning method. The YOLOv3 was proposed as a real-time object identification module to differentiate solid trash. YOLOv3 and YOLOv3-tiny were trained to recognize plastic bottles, paper, and metal with 150 distinct photos. When compared to the YOLOv3-tiny model, the YOLOv3 model produced a notably higher mean average precision (mAP). Due to the greater numbers of the convolutional layer in the latter, the mAP for YOLOv3 was averaging 81.93%, whereas the mAP for YOLOv3 was only 74.58%. Furthermore, the YOLOv3 convergence rate was also substantially faster than that of YOLOv3-tiny. YOLOv3 object detection accuracy for plastic bottles, paper, and metal is 0.98, 1, and 1, respectively, under static conditions. YOLOv3-tiny, on the other hand, was unable to detect plastic bottles, with detection accuracies of just 0.68 and 0.94 for paper and metal cans, respectively. Such attributes feature YOLOv3 as a viable object recognition tool for use in a solid waste sorting system.   Keywords Solid waste management  solid waste  deep learning  waste segregation  and convolutional neural network   21.1 Introduction  Solid waste (SW), commonly known as municipal SW (MSW), is causing adverse socioeconomic impacts when not handled appropriately [1,2] . This issue is exacerbated as the global population and urbanization are increasing precipitously, leading to a higher waste production rate [3] . The agricultural, manufacturing industry, and energy production are some of the notable MSW contributors [4] . Electronic waste in the MSW such as a personal computer, phone, television, or washer could potentially cause heavy metal pollution [5] . Fig. 21.1   shows the typical composition of MSW found in Malaysia, breaking down to 45% organic material, 7% paper, 24% plastic, 4% wood, 6% of iron, and others [1] . MSW like paper, plastic, and metal can be recycled and reused [6] . SW management (SWM) system was introduced to assist in handling the MSW generated from every resource of hazardous waste. Fig. 21.2   shows the typical SWM process flow. Initially, the MSWs are gathered from various locations or sources utilizing fixed stations or house-to-house collection and then they are transported by haulage truck to the SWM facility [7,8] . These MSWs are subsequently segregated based on their categories. MSWs that require chemical treatment will be sent to the waste processing stage to reduce their harmful impacts on the environment. Disposal methods including landfill, compositing, and incineration are used at the final stage [5] . In all, collection, transportation, segregation, processing, and disposal of the SWM must all be arranged according to the  right procedures. Any SWM must rigorously follow proper and accurate treatment procedures for SW since improper handling will result in major environmental and societal problems including illness, air pollution, and greenhouse gases [9] . Many techniques have been used for waste segregation, and they can be mainly classified into two categories: direct sorting (Table 21.1  ) and indirect sorting (Table 21.2  ) [10,11] . Direct sorting is a technique that uses the characteristics of SW to separate or identify the required item directly, whereas indirect sorting uses a sensor to find the SW itself. According to research by Gundupalli et al. [10] , spectral imaging-based sorting has a high level of material variety, accuracy, performance, and dependability [12] . Nonetheless, the most common waste segregation method is still manual sorting [1] . It is the most common way of waste segregation but at the same time, it is the least effective way of waste segregation [1] . Furthermore, manual sorting might also bring about negative effects on the segregator, as they might be exposed to hazardous materials [13] .   Combining deep learning or machine learning techniques with image processing can inherently increase the efficiency and accuracy of spectral imaging-based sorting. According to the study by Piotr et al. [14] , accuracy may be increased from 90% to 96.7% by implementing deep learning in classification systems. Deep learning is being used to improve the present trash segregation approach and raise recovery rates, while reducing or replacing the need for human labor in manual segregation. Deep learning is a component of artificial intelligence. It has been advised for use in SVM because it can be trained to accurately distinguish solid trash from a collection of MSW using the image processing approach [15] . Increasing the number of layers inside a deep learning machine’s architecture can enhance speed. Convolutional neural networks (CNNs) are the most often used architecture. As shown in Fig. 21.3  , the layers that make up a CNN for feature extraction include the input layer, convolution layer, pooling layer, and fully connected layer for output [16] . A three-dimensional feature map will be produced by splitting the image into two to three layers using the input layer [17] . The matrix will next be processed after going through the pooling layer, which assists in filtering out the lowest value by processing the map using a convolution operation. The size of the map will then be further reduced by a pooling layer. The pooling layer will use the maximum or average pooling algorithms to compute each region on the map. Finally, a feature map will be formed in the final connected layer. By using the output, it can use to compare the data with other images to classify objects on this image. In order to increase its accuracy and capability to recognize various types of SW objects using its own intricate architecture, deep learning may be applied to SVM in the form of object detection. This study  aims to establish a YOLOv3 and YOLOv3-tiny-based SW sorting system. The performance of the SW sorting system when operated under these deep learning schemes will also be evaluated subsequently. This system is expected to advance the use of deep learning methods in SW sorting systems, which in turn minimize the dependence on the human sorting system in a foreseeable future.  21.2 Methods 21.2.1 Conceptual design of a waste segregation machine Fig. 21.4   shows the conceptual design of the SW sorting machine. A conveyor belt, camera, Arduino Uno, and two servo motors with rods are used in the design and construction of this system. When the instruction is given, the servo motor with a rod will direct the SW to the appropriate bin while it is still going ahead. The appropriate bin will be slid over the solid garbage. The solid trash will be transported on a conveyor belt and scanned by a camera to determine if it is made of paper, metal, or plastic. When asked for paper, the first servo will reply, followed by servo 2. Plastic will be found, but it will be disposed of 3. The flowchart of the SW sorting machine operation is shown in Fig. 21.5  , while Fig. 21.6   shows the computer-aided drawing (CAD) for the waste sorting machine prototype.   21.2.2 Comparison of the object detection module Region-based convolution neural networks, like R-CNN, and regression-based convolution neural networks, like YOLO, are two types of object identification that use deep learning [20] . One of the widely used techniques for object detection is R-CNN. The operating idea of this objective detection is based on selective search, and the bounding box was constructed using the regression concept. Fast R-CNN is a development of R-CNN, which is another variant of R-CNN. R-CNN and  Fast R-CNN vary in that the latter uses a more performant network known as a VGG16-based network [21] . The YOLOv3 method, a regression-based convolution neural network, uses CNN to detect objects. After extensive investigation and analysis, YOLOv3 was selected as the object detecting method. YOLOv3 has a greater mean value of average accuracy, mAP, and recall rate when compared to R-CNN. R-CNN has an mAP of 69.7%, whereas YOLOv3 has 76.1%. [22] . When compared to R-CNN and Fast R-CNN, the YOLO also performs faster [20,23] . Another drawback of employing R-CNN is that, because of the time requirements for training, it cannot be used for real-time scanning. In the meanwhile, using the Fast R-CNN is not recommended because its performance will decline over time. The YOLOv3 will be the finest choice in this study as a result of the research. The YOLOv4 and YOLOv5 will not be selected due to the hardware limitations in this study since they required a better version. In comparison to the first version of YOLO, such as YOLOv2, YOLOv3 has greater accuracy and detection speed. As a result, the YOLOv3  approach, which is in charge of object identification and classification in this study, will be the best one to utilize. Fig. 21.7   shows the architecture of YOLOv3. The YOLOv3 has its type of neural network structure and uses the feature extraction network of DarkNet-53 (Fig. 21.8  ) as the backbone for YOLOv3. DarkNet-53 is made up of 53 layers of convolutional layer [24] . The YOLOv3 has a  total of 106 layers with 53 layers from the DarkNet-53, while another 53 layers are for detection purposes [25] . There are 5 different scales of the residual layer which is made up of 3×3 and 1×1 of the convolutional layer. The image will pass through the residual blocks to reduce the spatial resolution of the image to 1/32 of the input image, and then it will pass to another layer. The upsampling layer is a simple layer that doubles up the dimensions of the input. Two upsampling layers will take different inputs from the DarkNet-53 layer, which are from the third and fourth residual layers. In the end, there will be three different outputs for the detection layer responsible for the detection of the output in the form of feature maps on a different scale, which are 13×13, 26×26, and 52×52, which will pass to the detection layer [26,27] . As a result, three different output layers are y1, y2, and y3 [25,27] . At the end of the process, the output vector can be obtained or returned to the system for classification purposes. The output vector is shown in Eq. (21.1) , where P c   is prediction probability; B x   is x position for the center of the bounding box; B y   is y position for the center of the bounding box; B h   is the height of the bounding box; B w   is the weight of bounding box.; and C n   is class probabilities (n =number of class) [20,29] .(21.1) y = P  c     B  x     B  y     B  h     B  w     C  1     C  2     C  3             If there is an object in the picture, the P c   will be 1, or else it will be maintaining 0. If there is an object in the picture, the bounding box will be formed in the image, video, or real-time video and capture the whole object using the box. The parameter of the bounding box like weight, location, and height will be based on the value of B x  , B y  , B h  , and B w   (Eq. 21.2 ) based on the bounding box prior or anchor box which are the height, P h  , and the width, P w  [20,22,29] . To compute this value, there are some calculations by using the value of coordinates from the YOLOv3 network. The four coordinates are t x  , t y  , t h  , and t w  . The initiate coordinate  in the image is not located in the middle, it falls on the top left corner of the image called C x   and C y   and is divided into S×S grid cells.(21.2) B x  = σ t x   + C x    B y  = σ t y   + C y    B w  = p w  e t w      B h  = p h  e t h          The anchor box (Fig. 21.9  ) also plays an important role in the YOLOv3. It is used to detect the target depending on the k-mean. The anchor box is used to prepredict the object in the picture and predict multiple objects in the image if there is an object in the image. It will also help to identify two overlapping objects without considering them as one object. It will return the value as Pw and Pw. There will be three anchor boxes used during the detection and classification phase to target the object using a different scale: (10×13), (16×30), (33×23), (30×61), (62×45), (59×119), (116×90), (156×198), and (373×326) [22,30] . In general, the anchor box will be applied or appear on the image during the training process based on the value of intersection over union (IoU). IoU is an evaluation method in object detection, which is used for measuring accuracy. It will compare the value of the anchor box and ground truth box, which is the box where the image will fall in. The  higher the value of IoU, the greater the accuracy. The mathematical formula of IoU is shown in Eq. 21.3 .(21.3) IoU = A  a   ∩ B  t    A  a   ∪ B  t        where A a   is the area of the anchor box, while B t   is the area of the truth box. This equation is used to calculate the IoU (Fig. 21.10  ) [29,33,34] . If the object inside the anchor box is less than 0.5, then the anchor box will not take it as an object and no object will be detected and classified [29,33,34] .  21.2.3 Training YOLOv3 In this study, the data set will be prepared to train the YOLOv3 to classify the SW based on its type, which is plastic, metal, and paper. At the end of the training using the program Google Colab, the program will generate the WEIGHTS file and CFG file for this SVM. These files will then transfer to the program for running the object detection. There will have three sets of data of plastic, paper, and metal. Each set will contain 150 images for training purposes. Each image will be processed using software,  “labellmg,” to label each object in the picture and then generate a txt file (Fig. 21.11  ). This “txt” file will have the same name as the image file’s name, and it will contain the data for each object in the picture. It will contain the number of classes, the location of the class, and the size of the bounding box for that object. There will be five numbers in a row (Fig. 21.12  ). The number beginning with 0 or 1 will be the class number. In this case, the number 0 will be plastic, 1 will be metal, and 2 will be paper. The second (green) and third (red) numbers will be the x and y coordination. The last two numbers will be the width (yellow) and height (brown) of the bounding box. After labeling for all images, those sets will be combined and compressed into a zip file to upload to google drive. Due to the limitation of hardware, the training program and evaluation will be conducted using an online GPU, Google Colab to run the training process by using the DarkNet-53. Google Colab, also known as Google Colaboratory, allows for programming and executing the Python program in the browser for  12 hours. At the beginning of the program, it will access Google Drive and download the image zip file. Then the Darknet-53 will be downloaded to the Google Colab. The Google Colab will start to train using the image file and their txt file for training purposes. At the end of the training section in the Google Colab, there will have two files being generated, which are the CFG file and the WEIGHTS file for transfer learning purposes. The CFG file is the layout of the YOLOv3. In this case, the YOLOv3 will be modified as three classes need to be detected so the layout has been modified. To check whether the trained model is accurate, these two things will be evaluated, which are the mean average precision, mAP, precision, recall, and loss. These data will be the judge for the performance of the YOLOv3 training phase. The precision is showing that the ratio of the number of objects detected is correct, which can be expressed as Eq. 21.4 , where N TP   is the number of truth positives (number of objects detected correctly) and N FP  is the number of false positives (number of objects detected wrongly) [29,35,36] .(21.4) Precision = N  TP    N  TP   + N  FP         The recall rate  indicates the rate of predicting an object wrongly. If the rate is small, the object detection model is predicted correctly to all data [29,35,36] . The recall rate  is expressed by Eq. 21.5 , where N FN   is the number of false negatives (number of ground truth objects not detected) [29,33,36] .(21.5) Recall = N  TP    N  TP   + N  FN         The mAP stands for the object in the images that are detected correctly. It is the mean value based on the calculation of average precision for all classes. The average precision (AP) can be obtained using the curve of precision recall by calculating the area under the curve. Then the value will be passed to another formula to calculate the mAP, which is expressed by Eq. 21.6 , where n is the number of classes [29,33] .(21.6) mAP = 1  n   × ∑ i = 1  N   A P  i        The loss value is given at last. There are three types of loss which are classification loss (Eq. 21.7 ), localization loss (Eq. 21.8 ), and confidence loss (Eq. 21.9 ). If the YOLO performs unsatisfactorily, like if the  prediction is often wrong, the loss value will be higher. If the YOLO always predicts the object correctly, the loss value will be very small. The ideal loss value is lesser than 1 to ensure that the YOLOv3 model performs effectively and correctly. The localization loss is the error of the prediction box and the ground truth object in which the prediction box is offset from the actual object [29,30,37] . The confidence loss is the error of objectness of the box, which supposes that there is an object in the box, but the system did not detect so it will consider as confidence loss [29,30,37] . The last loss will be the classification loss, which is the error occurring when misclassification happens [29,30,37] . All types of loss added together will become the total loss, as shown in Eq. 21.10 .(21.7) Classification Loss = ∑ i = 0  s  2     ∑ c ∈ classes   [ p  i   c    −   p  ˆ   i   c    ]  2      where c  is the specified class, p  ˆ   i     is true value probability for the classes, and p  i     is predicted value probability for the classes [23,29,30,37] .(21.8) Localization = λ  coord   ∑ i = 0  s  2    ∑ j = 0  NB   l  i j  obj   [ ( x  i   − x ˆ   i    )   2  + ( y  i   − y ˆ   i    )   2   ]    + λ  coord   ∑ i = 0  s  2    ∑ j = 0  NB   l  i j  obj   [ ( w  i     − w ˆ   i     )  2  + ( h  i     − h ˆ   i     )  2   ]        where x  ˆ   i   , y  ˆ   i   , w  ˆ   i   , h  ˆ   i     is coordination, width, and height of bounding box, x  i   , y  i   , w  i   , h  i     is the truth value of the bounding box, and λ  coord     is coordinate error weight, S  is the number of grids, and NB  is the number of bounding boxes [23,29,30,37] .(21.9) Confidence Loss = ∑ i = 0  s  2     ∑ j = 0  B   l  ij  obj   ( c  i   − c  ˆ   i   )  2     + λ  noobj   ∑ i = 0  s  2     ∑ j = 0  B   l  ij  obj   ( c  i   − c  ˆ   i   )  2        where λ  noob     is IoU error weight, c  ˆ   i     is truth confidence value, and c  i     is predict confidence value [23,29,30,37] .(21.10) Total Loss = Localization Loss + Confidence Loss + Classification Loss     At the end of the training of YOLOv3, the loss and mAP graph will be evaluated, and the result will be compared with YOLOv3-tiny with  the same dataset but different configuration to see which model is better before being transferred to the computer to ensure that the model is able to get trained well and able to perform effectively. Those two files generated from the training will be transferred to the computer to run the file for object detection.   21.3 Results and discussions The focus of this section is to evaluate the accuracy of the object detection model during the training process. This is essential to ensure that the system is capable of carrying out its duty. The loss and map graph will be examined comprehensively. This section will focus on training the YOLOv3 and YOLOv3-tiny and examining their performances. The variables that affect the mAP, recall, and loss during object detection will be investigated. Google Colab with inherently limited runtime will be utilized to perform the training process. A total of 450 images will be used for the training due to the limitation of Google Colab. During the training process, 80% of the image will be used for training, while 20% of the image will be used to validate whether the trained model can detect the object inside the image. The total number of images is restricted to 450 due to the total number of images for training and testing purposes cannot be more than 500 in Google Colab. 21.3.1 Evaluation of Training The model will detect the object inside the image and then export the class, location, and the bounding box of the object; then these data will be used to compare with the data file that already consists of the data of that image and then come out with the precision, F1-score, recall rate, true positive, false positive, false negative, an average of IoU, and mAP. F1-score is the harmonic mean of precision and recall, the formula of which is given by Eq. 21.11 [38] .(21.11) F 1 − score = 2 × Precision × Recall  Recall × Precision       The F1-score is also a parameter to judge the model and whether the model is accurate or not. The value of F1-score is between zero and one. If the value is 1, then the model has high accuracy, while if the value is 0, then it means the model is not accurate enough to detect an object. During this training, the mAP of 50% will be used as a minimum score  for the evaluation, which means if the object detected inside the image has more confidence, more than 50% or 0.5 will be taken as true and the result of the training model will be compared with the actual answer. The table given above is the result of each 1000 iteration, which means the number of times it is trained. A highly accurate model will have high F1-score, TP, IoU, and mAP, while low in FP and FN. TP will indicate the object is detected by the model and is correct to the ground truth, while FP means the object detects the object in the image, but there is no object inside the bounding box [29,35,36] . FN means object exists in the image but the model is unable to detect the object. Table 21.3   shows that the parameter from YOLOv3 is usually higher than the result of YOLOv3-tiny. This can be explained by the convolutional layer being different compared to YOLOv3 with YOLOv3-tiny. YOLOv3-tiny has a lesser convolutional layer compared to the YOLOv3. The convolutional layer helps to filter out the input image and detect the presence of an object in the image if the filter is less, which means that it is unable to detect the object inside the image accurately. Although both models are using the same darkent53. conv.74 to train the dataset, their configuration file is still different compared to others. The cfg file of the model contains the structure of the model. YOLOv3 has a structure of darknet 53, while the YOLOv3-tiny just has seven basic convolutional layers and some 1×1 and 3×3 convolutional layers [39] . Their algorithm of YOLOv3 and YOLOv3-tiny are different, which leads to the rest of the results in the table given above being different from each other. Based on Table 21.3 , it can be seen that YOLOv3 has higher accuracy with respect to the F1-score, mAP and IoU are more than YOLOv3-tiny, while YOLOv3 has just 0.01 accuracy. The YOLOv3-tiny cannot detect the object in the test image with just training 1000 iterations as it returns the result of 292 in FN. After training 2000 iteration, both models are more accurate compared to the models that just trained 1000. This shows that the number of iterations will affect the parameter. This does not much affect the YOLOv3 model, as it has an mAP of more than 80% on the 2000 iteration. Comparing YOLOv3 to YOLOv3-tiny, YOLOv3’s result showed that it is more accurate than the YOLOv3-tiny based on F1-score, TP, FP, FN, IoU, and mAP. YOLOv3 has 81.93% of mAP, which is more than 74.58% of mAP of YOLOv3-tiny. YOLOv3 also has 0.83 of F1-score, while F1-score of YOLOv3-tiny is just 0.75. It can be concluded that YOLOv3 has more accuracy compared to YOLOv3-tiny, and the more convolutional layer of the model, the more accurate is the model.    21.3.2 Training results for classes In this subsection, the results for the accuracy of models when detecting the object in each class are shown. This result assesses whether the model can classify the object correctly or not. Table 21.4   shows the result for  each training result for each class after 1000 iterations of training. Based on the result mentioned above, it can be seen that YOLOv3 has much more accuracy for each class compared to the YOLOv3-tiny based on the AP, TP, and FP. Using the last result of training at 6000 iterations, YOLOv3 has an AP of 90.78% in plastic class, 74.72% in paper class, while 80.31% in metal class, compared to YOLOv3-tiny’s AP results in plastic, paper, and metal class, which are 83.99%, 68.95%, and 70.80%, respectively. It is clearly shown that YOLOv3 has more AP in these three different classes. The TP in the last training of each class of YOLOv3 also is much higher compared to that of YOLOv3-tiny, while YOLOv3 has much lesser FP in plastic and metal. As the result showed again, YOLOv3 has much higher accuracy in classification ability, which is going to implement in object detection to separate the SW based on the type.  21.3.3 Training loss result To determine if object detection is highly accurate or stop the training of the object detection model, one of the methods is to observe the loss during training. As mentioned above, the loss is a type of error in the object detection model, which indicates that the object detection model is unable to detect, locate, and classify the object in the image [29,30,37] . The higher loss means the model is unable to perform well, while the lower loss means the object detection model can detect, locate, and classify the object in the image. In this section, the loss graph of two models will be generated and loss in each 1000 training has been recorded to compare the reason. Based on Figs. 21.13 and 21.14   , it is shown that the loss curve of YOLOv3 converges faster than the YOLOv3-tiny, although the YOLOv3 has more loss in the beginning, which is 2116.475098, while the loss of YOLOv3-tiny in the beginning is just 678.747498. From the graph and Table 21.5  , the YOLOv3’s loss drops much faster than the YOLOv3-tiny, as both learning rates are the same. This showed that the YOLOv3 can train much faster than YOLOv3-tiny to achieve much higher accuracy, even though both starting losses are different. The result also showed that the convolutional layer of YOLOv3 helped the model to get a much lower low at 1000 iterations, while YOLOv3-tiny loss is higher than the loss of YOLOv3 at 1000 iterations. This showed that YOLOv3 is much more suitable to use to train a large dataset and has better accuracy with lesser iterations, while Yolv3-tiny is not. As the training   continues, the loss of both models keeps decreasing, and so the loss of YOLOv3 will be 0.123053, while YOLOv3-tiny is 0.964706, which means YOLOv3 is much more accurate compared to YOLOv3-tiny. Based on the result, YOLOv3’s training is much more effective than the training of YOLOv3-tiny, as YOLOv3 will just need to train 1000 to get 0.863667, while YOLOv3-tiny required more iteration like the result of 6000 iteration to get 0.964706. Figs. 21.15 and 21.16    show the loss and mAP relationship. As the loss value keeps decreasing, the mAP will also increase. It is clearly shown in Figs. 21.15 and 21.16  that the YOLOv3 model converges faster in mAP and loss curves, while YOLOv3-tiny does not. The mAP curve is plotted to start from 1000 iterations and after every 100 iterations will be calculated again and plotted in the graph. The mAP of YOLOv3-tiny is highly unstable, as it drops rapidly and raises rapidly, which indicates that the model required more training to be stable. Once the YOLOv3-tiny model reaches 4800 iterations, the curve will start to be stable and the mAP value will be around 77%. As a result, YOLOv3 is hypothesized to be more accurate than YOLOv3-tiny.  21.3.4 Testing image and prototype After getting the result from the training, the testing will further proceed to test the models with the image to determine whether the model can detect and perform well before being applied to the hardware. The objective of this testing is to find out whether the trained models can detect the object that has been trained in the training to classify the object based  on their type which is to achieve one of the objectives of this paper. As the result, the performance will be compared and evaluated. In this test, the image of plastic, paper, and metal will be used as samples for testing. As shown in Figs. 21.17 and 21.18   , YOLOv3 can detect that there are three different plastic bottles and can locate them, along with classifying the object, while the YOLOv3-tiny is unable to detect those plastic bottles. Although YOLOv3-tiny can detect the paper and metal can in the picture, YOLOv3 can locate them more accurately and its confidence score is much higher than that of YOLOv3-tiny. As a result, YOLOv3 is much more accurate compared to YOLOv3-tiny. Likewise for the paper (Figs. 21.19 and 21.20   ) and metal can testing (Figs. 21.21 and 21.22   ), YOLOv3 has outperformed YOLOv3-tiny in terms of accuracy. This   model is then transferred to the waste sorting machine. As shown in Fig. 21.23  , the YOLOv3 can recognize the metal can, despite with lower accuracy than in static conditions.      21.4 Conclusion This paper examines the use of YOLOv3 and YOLOv3-tiny for an SVM system that can replace the current issue of waste segregation by manual picking and resource recovery to reduce the waste of materials that can reuse or remake for another item. The deep learning method was used to train the system using images of three categories SWs (plastic, paper, and metal). The YOLOv3 model has a higher mAP value compared to the YOLOv3-tiny, as the YOLOv3 has an mAP of 81.93%, while YOLOv3-tiny has an mAP of only 74.58%, because YOLOv3 has more convolutional layers compared to YOLOv3-tiny. This showed that YOLOv3 is more accurate compared to YOLOv3-tiny. The training of YOLOv3 is also faster than YOLOv3-tiny as if the YOLOv3-tiny needs to catch up with the mAP of YOLOv3. As a result, YOLOv3 can be trained with the images and can achieve an accuracy of up to 81.93%. After the complete training of YOLOv3, it can perform object detection for plastic, paper, and metal; the accuracy for each class will be 90.78%, 74.71%, and 80.31%. When applying the YOLOv3 to the waste sorting machine, it can also detect SW on a moving conveyor belt accurately, although the accuracy is notably lower than at static conditions. As for the future works, the ability of the object recognition system should be enhanced to recognize more classes. Furthermore, the ability to sort a large number of wastes at the same time should also be incorporated.   References [1] O.C. Aja  H.H. Al-Kayiem   Review of municipal solid waste management options in Malaysia, with an emphasis on sustainable waste-to-energy options   J. Mater. Cycles Waste Manage.  16  2014  693 710   https://doi.org/10.1007/s10163-013-0220-z    Aja O.C., Al-Kayiem H.H. Review of municipal solid waste management options in Malaysia, with an emphasis on sustainable waste-to-energy options. J. Mater. Cycles Waste Manage. 2014;16:693–710. https://doi.org/10.1007/s10163-013-0220-z.  [2] N. Vongdala  H.-D. Tran  T.D. Xuan  R. Teschke  T.D. Khanh   Heavy metal accumulation in water, soil, and plants of municipal solid waste landfill in vientiane   Laos. Int. J. Env. Res. Public Health  16  2018   https://doi.org/10.3390/ijerph16010022    Vongdala N., Tran H.-D., Xuan T.D., Teschke R., Khanh T.D. Heavy metal accumulation in water, soil, and plants of municipal solid waste landfill in vientiane, Laos. Int. J. Env. Res. Public Health 2018;16. https://doi.org/10.3390/ijerph16010022.  [3] S. Das  S.-H. Lee  P. Kumar  K.-H. Kim  S.S. Lee  S.S. Bhattacharya   Solid waste management: scope and the challenge of sustainability   J. Clean. Prod.  228  2019  658 678   https://doi.org/10.1016/j.jclepro.2019.04.323    Das S., Lee S.-H., Kumar P., Kim K.-H., Lee S.S., Bhattacharya S.S. Solid waste management: scope and the challenge of sustainability. J. Clean. Prod. 2019;228:658–678. https://doi.org/https://doi.org/10.1016/j.jclepro.2019.04.323.  [4] V. Saravanan, Critical review on the solid-wastes issue: generation, composition, disposal and their recycling potential for various applications, n.d. https://doi.org/10.1088/1742-6596/1804/1/012147 .   [5] H. Ma  Y. Cao  X. Lu  Z. Ding  W. Zhou   Review of typical municipal solid waste disposal status and energy technology   Energy Procedia  88  2016  589 594   https://doi.org/10.1016/j.egypro.2016.06.083    Ma H., Cao Y., Lu X., Ding Z., Zhou W. Review of typical municipal solid waste disposal status and energy technology. Energy Procedia 2016;88:589–594. https://doi.org/https://doi.org/10.1016/j.egypro.2016.06.083.  [6] I.A. Jereme  M. MahmudulAlam  C. Siwar   Waste recycling in Malaysia: transition from developing to developed country   Indian J. Educ. Inf. Manage.  4  2015  1 22    Jereme I.A., MahmudulAlam M., Siwar C. Waste recycling in Malaysia: transition from developing to developed country. Indian J. Educ. Inf. Manage. 2015;4:1–22.  [7] A.V. Shekdar   Sustainable solid waste management: an integrated approach for Asian countries   Waste Manage.  29  2009  1438 1448   https://doi.org/10.1016/j.wasman.2008.08.025    Shekdar A.V. Sustainable solid waste management: an integrated approach for Asian countries. Waste Manage. 2009;29:1438–1448. https://doi.org/https://doi.org/10.1016/j.wasman.2008.08.025.  [8] N. Parvez  A. Agrawal  A. Kumar   Solid waste management on a campus in a developing country: a study of the indian institute of technology Roorkee   Recycling  4  2019  20 28   https://doi.org/10.3390/recycling4030028    Parvez N., Agrawal A., Kumar A. Solid waste management on a campus in a developing country: a study of the indian institute of technology Roorkee. Recycling 2019;4:20–28. https://doi.org/10.3390/recycling4030028.  [9] N. Ferronato  V. Torretta   Waste mismanagement in developing countries: a review of global issues   Int. J. Env. Res. Public Health  16  2019   https://doi.org/10.3390/ijerph16061060    Ferronato N., Torretta V. Waste mismanagement in developing countries: a review of global issues. Int. J. Env. Res. Public Health 2019;16. https://doi.org/10.3390/ijerph16061060.  [10] S.P. Gundupalli  S. Hait  A. Thakur   A review on automated sorting of source-separated municipal solid waste for recycling   Waste Manage.  60  2017  56 74   https://doi.org/10.1016/j.wasman.2016.09.015    Gundupalli S.P., Hait S., Thakur A. A review on automated sorting of source-separated municipal solid waste for recycling. Waste Manage. 2017;60:56–74. https://doi.org/https://doi.org/10.1016/j.wasman.2016.09.015.  [11] S.K. Siddappaji, R.C. Radha, Technologies for segregation and management of solid waste: a review, in: 2016 Int. Conf. Emerg. Trends Eng. Technol. Sci., 2016, pp. 1–4. https://doi.org/10.1109/ICETETS.2016.7603046 .   [12] O. Adedeji  Z. Wang   Intelligent waste classification system using deep learning convolutional neural network   Procedia Manuf.  35  2019  607 612   https://doi.org/10.1016/j.promfg.2019.05.086    Adedeji O., Wang Z. Intelligent waste classification system using deep learning convolutional neural network. Procedia Manuf. 2019;35:607–612. https://doi.org/https://doi.org/10.1016/j.promfg.2019.05.086.  [13] J. Gutberlet  S.M.N. Uddin   Household waste and health risks affecting waste pickers and the environment in low- and middle-income countries   Int. J. Occup. Environ. Health  23  2017  299 310   https://doi.org/10.1080/10773525.2018.1484996    Gutberlet J., Uddin S.M.N. Household waste and health risks affecting waste pickers and the environment in low- and middle-income countries. Int. J. Occup. Environ. Health 2017;23:299–310. https://doi.org/10.1080/10773525.2018.1484996.  [14] P. Nowakowski  T. Pamuła   Application of deep learning object classifier to improve e-waste collection planning   Waste Manage.  109  2020  1 9   https://doi.org/10.1016/j.wasman.2020.04.041    Nowakowski P., Pamuła T. Application of deep learning object classifier to improve e-waste collection planning. Waste Manage. 2020;109:1–9. https://doi.org/https://doi.org/10.1016/j.wasman.2020.04.041.  [15] F. Eleonu  P. Anebo   An improved model for waste management recommender system in rivers state using deep learning approach   Int. J. Comput. Trends Technol.  68  2020  193 203    Eleonu F., Anebo P. An improved model for waste management recommender system in rivers state using deep learning approach. Int. J. Comput. Trends Technol. 2020;68:193–203.  [16] H. Mohsen  E.-S.A. El-Dahshan  E.-S.M. El-Horbaty  A.-B.M. Salem   Classification using deep learning neural networks for brain tumors   Futur. Comput. Inform. J.  3  2018  68 71   https://doi.org/10.1016/j.fcij.2017.12.001    Mohsen H., El-Dahshan E.-S.A., El-Horbaty E.-S.M., Salem A.-B.M. Classification using deep learning neural networks for brain tumors. Futur. Comput. Inform. J. 2018;3:68–71. https://doi.org/https://doi.org/10.1016/j.fcij.2017.12.001.  [17] H. Chen  A. Chen  L. Xu  H. Xie  H. Qiao  Q. Lin    A deep learning CNN architecture applied in smart near-infrared analysis of water pollution for agricultural irrigation resources   Agric. Water Manage.  240  2020  106303   https://doi.org/10.1016/j.agwat.2020.106303    Chen H., Chen A., Xu L., Xie H., Qiao H., Lin Q., et al. A deep learning CNN architecture applied in smart near-infrared analysis of water pollution for agricultural irrigation resources. Agric. Water Manage. 2020;240:106303. https://doi.org/https://doi.org/10.1016/j.agwat.2020.106303.  [18] H. Wang  A. Cruz-Roa  A. Basavanhally  H. Gilmore  N. Shih  M. Feldman    Mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features   J. Med. Imaging (Bellingham, Wash.)  1  2014  34003   https://doi.org/10.1117/1.JMI.1.3.034003    Wang H., Cruz-Roa A., Basavanhally A., Gilmore H., Shih N., Feldman M., et al. Mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features. J. Med. Imaging (Bellingham, Wash.) 2014;1:34003. https://doi.org/10.1117/1.JMI.1.3.034003.  [19] V. Maeda-Gutiérrez  C.E. Galván-Tejada  L.A. Zanella-Calzada  J.M. Celaya-Padilla  J.I. Galván-Tejada  H. Gamboa-Rosales    Comparison of convolutional neural network architectures for classification of tomato plant diseases   Appl. Sci.  10  2020   https://doi.org/10.3390/app10041245    Maeda-Gutiérrez V., Galván-Tejada C.E., Zanella-Calzada L.A., Celaya-Padilla J.M., Galván-Tejada J.I., Gamboa-Rosales H., et al. Comparison of convolutional neural network architectures for classification of tomato plant diseases. Appl. Sci. 2020;10. https://doi.org/10.3390/app10041245.  [20] S. Kumar  D. Yadav  H. Gupta  O.P. Verma  I.A. Ansari  C.W. Ahn   A novel YOLOv3 algorithm-based deep learning approach for waste segregation: towards smart waste management   Electronics  10  2021   https://doi.org/10.3390/electronics10010014    Kumar S., Yadav D., Gupta H., Verma O.P., Ansari I.A., Ahn C.W. A novel YOLOv3 algorithm-based deep learning approach for waste segregation: towards smart waste management. Electronics 2021;10. https://doi.org/10.3390/electronics10010014.  [21] Y. Xiao  X. Wang  P. Zhang  F. Meng  F. Shao   Object detection based on faster R-CNN algorithm with skip pooling and fusion of contextual information   Sensors  20  2020   https://doi.org/10.3390/s20195490    Xiao Y., Wang X., Zhang P., Meng F., Shao F. Object detection based on faster R-CNN algorithm with skip pooling and fusion of contextual information. Sensors 2020;20. https://doi.org/10.3390/s20195490.  [22] H. Yang  P. Liu  Y. Hu  J. Fu   Research on underwater object recognition based on YOLOv3   Microsyst. Technol.  27  2021  1837 1844   https://doi.org/10.1007/s00542-019-04694-8    Yang H., Liu P., Hu Y., Fu J. Research on underwater object recognition based on YOLOv3. Microsyst. Technol. 2021;27:1837–1844. https://doi.org/10.1007/s00542-019-04694-8.  [23] J. Xiao   ScienceDirect ScienceDirect exYOLO: a small object detector based on YOLOv3 Object exYOLO: a small object detector based on YOLOv3 object detector detector CQVIP conference on data driven intelligence and innovation   Procedia Comput. Sci.  188  2021  18 25   https://doi.org/10.1016/j.procs.2021.05.048    Xiao J. ScienceDirect ScienceDirect exYOLO: a small object detector based on YOLOv3 Object exYOLO: a small object detector based on YOLOv3 object detector detector CQVIP conference on data driven intelligence and innovation. Procedia Comput. Sci. 2021;188:18–25. https://doi.org/10.1016/j.procs.2021.05.048.  [24] J. Redmon  A. Farhadi   YOLOv3: an incremental improvement   Comput. Sci.   2018   abs/1804.0  Redmon J., Farhadi A. YOLOv3: an incremental improvement. Comput. Sci. 2018;abs/1804.0.  [25] K. Alderliesten, YOLOv3—Real-time object detection. https://medium.com/analytics-vidhya/yolov3-real-time-object-detection-54e69037b6d0 , 2020.   [26] J. Liu  D. Zhang   Research on vehicle object detection algorithm based on improved {YOLOv}3 algorithm   J. Phys. Conf. Ser.  1575  2020  12150   https://doi.org/10.1088/1742-6596/1575/1/012150    Liu J., Zhang D. Research on vehicle object detection algorithm based on improved {YOLOv}3 algorithm. J. Phys. Conf. Ser. 2020;1575:12150. https://doi.org/10.1088/1742-6596/1575/1/012150.  [27] S. Islam, YOLOv3 object detection. https://inverseai.com/blog/yolov3-object-detection , 2020.   [28] M. Ju  H. Luo  Z. Wang  B. Hui  Z. Chang   The application of improved YOLO V3 in multi-scale target detection   Appl. Sci.  9  2019   https://doi.org/10.3390/app9183775    Ju M., Luo H., Wang Z., Hui B., Chang Z. The application of improved YOLO V3 in multi-scale target detection. Appl. Sci. 2019;9. https://doi.org/10.3390/app9183775.  [29] B.V. Kumar  S. Abirami  R.J.B. Lakshmi  R. Lohitha  R.B. Udhaya   Detection and content retrieval of object in an image using {YOLO}   {IOP} Conf. Ser. Mater. Sci. Eng.  590  2019  12062   https://doi.org/10.1088/1757-899x/590/1/012062    Kumar B.V., Abirami S., Lakshmi R.J.B., Lohitha R., Udhaya R.B. Detection and content retrieval of object in an image using {YOLO}. {IOP} Conf. Ser. Mater. Sci. Eng. 2019;590:12062. https://doi.org/10.1088/1757-899x/590/1/012062.  [30] J. Li  J. Gu  Z. Huang  J. Wen   Application research of improved YOLO V3 algorithm in PCB electronic component detection   Appl. Sci.  9  2019   https://doi.org/10.3390/app9183750    Li J., Gu J., Huang Z., Wen J. Application research of improved YOLO V3 algorithm in PCB electronic component detection. Appl. Sci. 2019;9. https://doi.org/10.3390/app9183750.  [31] S. Kumar, D. Yadav, H. Gupta, O.P. Verma, I.A. Ansari, C.W. Ahn, 2020. A novel YOLOv3 algorithm-based deep learning approach for waste segregation: Towards smart waste management. Electronics 10 (2021) 14.   [32] R. Adrian. Intersection over Union (IoU) for object detection [Online]. https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection , 2016 (accessed 20.06.2021).   [33] B. Bhatt, The mystery of true positive, true negative, false positive and false negative. https://medium.com/@bhattbhavesh91/the-mystery-of-true-positive-true-negative-false-positive-and-false-negative-fd73c78c905a , 2017.   [34] A. Rosebrock, Intersection over Union (IoU) for object detection. https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ , 2016.   [35] M. Khoshdeli, R. Cong, B. Parvin, Detection of nuclei in H&E stained sections using convolutional neural networks, in: 2017 IEEE EMBS Int. Conf. Biomed. Heal. Informatics, 2017, pp. 105–108. https://doi.org/10.1109/BHI.2017.7897216 .   [36] W. He  Z. Huang  Z. Wei  C. Li  B. Guo   TF-YOLO: an improved incremental network for real-time object detection   Appl. Sci.  9  2019   https://doi.org/10.3390/app9163225    He W., Huang Z., Wei Z., Li C., Guo B. TF-YOLO: an improved incremental network for real-time object detection. Appl. Sci. 2019;9. https://doi.org/10.3390/app9163225.  [37] J. Hui, Real-time object detection with YOLO, YOLOv2 and now YOLOv3. https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088 , 2018.   [38] M. Park  B.C. Ko   Two-step real-time night-time fire detection in an urban environment using static ELASTIC-YOLOv3 and temporal fire-tube   Sensors  20  2020   https://doi.org/10.3390/s20082202    Park M., Ko B.C. Two-step real-time night-time fire detection in an urban environment using static ELASTIC-YOLOv3 and temporal fire-tube. Sensors 2020;20. https://doi.org/10.3390/s20082202.  [39] M. Zhang  W. Zhao  H. Li  F. Wang  S. Zhang   Research on the application of deep learning {YOLOv}3 in aerial patrol inspection of optical cable lines   J. Phys. Conf. Ser.  1345  2019  32068   https://doi.org/10.1088/1742-6596/1345/3/032068    Zhang M., Zhao W., Li H., Wang F., Zhang S. Research on the application of deep learning {YOLOv}3 in aerial patrol inspection of optical cable lines. J. Phys. Conf. Ser. 2019;1345:32068. https://doi.org/10.1088/1742-6596/1345/3/032068.         